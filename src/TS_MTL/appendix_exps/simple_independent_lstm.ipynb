{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def load_existing_results(file_path=\"forecasting_results.json\"):\n",
    "    \"\"\"\n",
    "    Load existing results from a JSON file.\n",
    "    Returns an empty dictionary if the file doesn't exist.\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "\n",
    "def save_results_to_json(data, file_path=\"forecasting_results.json\"):\n",
    "    \"\"\"\n",
    "    Save the results dictionary to a JSON file, handling NumPy data types.\n",
    "    \"\"\"\n",
    "\n",
    "    # Handle NumPy data types (recursive conversion)\n",
    "    def convert_numpy(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            return {k: convert_numpy(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [convert_numpy(i) for i in obj]\n",
    "        elif isinstance(obj, (np.integer, np.int64, np.int32)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.floating, np.float64, np.float32)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()  # Convert arrays to lists\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "    # Convert data and save to JSON\n",
    "    data = convert_numpy(data)\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "    print(f\"✅ Results saved to {file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "def store_results(dataset_name, horizons, horizon_value, experiment_type, backbone, mae_result, file_path=\"forecasting_results.json\"):\n",
    "    \"\"\"\n",
    "    Store MAE results for a given experiment type (stl_mae, mtl_mae, global_mae) per horizon.\n",
    "\n",
    "    Args:\n",
    "    - dataset_name (str): Name of the dataset (e.g., 'Solar', 'Air Quality').\n",
    "    - horizons (list): List of horizon values (e.g., [1, 2, 4, 8, 16]).\n",
    "    - horizon_value (int): The horizon corresponding to the mae_result provided.\n",
    "    - experiment_type (str): One of ['stl_mae', 'mtl_mae', 'global_mae'].\n",
    "    - backbone (str): Model backbone name (e.g., 'Deep_LSTM', 'simple_transformer').\n",
    "    - mae_result (list): MAE values for the current horizon (list of floats).\n",
    "    - file_path (str): JSON file to store the results.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Load existing results\n",
    "    results_dict = load_existing_results(file_path)\n",
    "\n",
    "    # Create dataset entry if it doesn't exist\n",
    "    dataset_key = f\"{dataset_name}_{backbone}\"\n",
    "    if dataset_key not in results_dict:\n",
    "        results_dict[dataset_key] = {\n",
    "            \"horizons\": horizons,\n",
    "            \"mtl\": [[] for _ in horizons],\n",
    "            \"global\": [[] for _ in horizons],\n",
    "            \"independent\": [[] for _ in horizons]\n",
    "        }\n",
    "\n",
    "    # Find index for the given horizon\n",
    "    try:\n",
    "        horizon_index = horizons.index(horizon_value)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"⚠️ Horizon value {horizon_value} not found in {horizons}.\")\n",
    "\n",
    "    # Append the mae_result to the correct horizon\n",
    "    results_dict[dataset_key][experiment_type][horizon_index].extend(mae_result)\n",
    "\n",
    "    # Save updated results\n",
    "    save_results_to_json(results_dict, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import dateutil.parser\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def df_to_X_y(df, features, target, window_size=32, horizon=1):\n",
    "    if target not in features:\n",
    "        features = [target] + features\n",
    "\n",
    "    data = df[features].to_numpy()\n",
    "    target_data = df[target].to_numpy()\n",
    "\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size - horizon + 1):\n",
    "        X.append(data[i:i + window_size])\n",
    "        y.append(target_data[i + window_size: i + window_size + horizon])\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def load_and_preprocess_site_data(site_path,features, target ,window_size=32, horizon=16, min_date=None, max_date=None, batch_size=16, device='cpu'):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses time series data for a given site.\n",
    "\n",
    "    Args:\n",
    "    - site_path (str): Path to the site CSV file.\n",
    "    - window_size (int): Number of past time steps for input.\n",
    "    - horizon (int): Number of future steps to predict.\n",
    "\n",
    "    Returns:\n",
    "    - train_loader, val_loader, test_loader: DataLoaders for training, validation, and testing.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(site_path)\n",
    "    \n",
    "    # Convert date column to datetime if it exists\n",
    "    if 'date' in df.columns:\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "        # Filter data between min_date and max_date\n",
    "        if min_date:\n",
    "            min_date = dateutil.parser.parse(min_date) if isinstance(min_date, str) else min_date\n",
    "            df = df[df['date'] >= min_date]\n",
    "\n",
    "        if max_date:\n",
    "            max_date = dateutil.parser.parse(max_date) if isinstance(max_date, str) else max_date\n",
    "            df = df[df['date'] <= max_date]\n",
    "\n",
    "        # Drop the date column after filtering\n",
    "        df.drop(columns=['date'], inplace=True)\n",
    "        \n",
    "    \n",
    "    # Perform an 80-20 split based on time order\n",
    "    train_size = int(0.8 * len(df))\n",
    "    train_df = df.iloc[:train_size]  # 80% for training & validation\n",
    "    test_df = df.iloc[train_size:]   # 20% for final testing (future unseen data)\n",
    "\n",
    "    # Split train_df further into Train (80%) and Validation (20%)\n",
    "    # val_size = int(0.2 * len(train_df))  # 16% of full dataset\n",
    "    # train_df, val_df = train_df.iloc[:-val_size], train_df.iloc[-val_size:]\n",
    "\n",
    "    print(f\"Train size: {len(train_df)} | Validation size: {len([])} | Test size: {len(test_df)}\")\n",
    "\n",
    "    # Standardize each separately to prevent data leakage\n",
    "    train_mean, train_std = train_df.mean(), train_df.std()\n",
    "    \n",
    "    train_df = (train_df - train_mean) / (train_std + 1e-8)\n",
    "    # val_df = (val_df - train_mean) / (train_std + 1e-8)  # Normalize validation using train stats\n",
    "    test_df = (test_df - train_mean) / (train_std + 1e-8)  # Normalize test using train stats\n",
    "\n",
    "    # Convert DataFrame to NumPy arrays for LSTM\n",
    "    X_train, y_train = df_to_X_y(train_df,features, target, window_size, horizon)\n",
    "    # X_val, y_val = df_to_X_y(val_df, features, target, window_size, horizon)\n",
    "    X_test, y_test = df_to_X_y(test_df,features, target, window_size, horizon)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    # train_data = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float())\n",
    "    # val_data = TensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).float())\n",
    "    # test_data = TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).float())\n",
    "    \n",
    "    train_data = TensorDataset(torch.tensor(X_train, dtype=torch.float32).to(device), torch.tensor(y_train, dtype=torch.float32).to(device))\n",
    "    # val_data = TensorDataset(torch.tensor(X_val, dtype=torch.float32).to(device), torch.tensor(y_val, dtype=torch.float32).to(device))\n",
    "    test_data = TensorDataset(torch.tensor(X_test, dtype=torch.float32).to(device), torch.tensor(y_test, dtype=torch.float32).to(device))\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "    # val_loader = DataLoader(val_data, shuffle=False, batch_size=batch_size, drop_last=True)\n",
    "    test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "    # return train_loader, val_loader, test_loader\n",
    "    return train_loader, test_loader\n",
    "\n",
    "# ---------- Data Loader Function with Detrending ----------\n",
    "def load_and_preprocess_site_data_detrend(site_path, features, target, window_size=32, horizon=1, \n",
    "                                          min_date=None, max_date=None, batch_size=16, device='cpu'):\n",
    "    df = pd.read_csv(site_path)\n",
    "\n",
    "    # Convert date column to datetime if it exists\n",
    "    if 'date' in df.columns:\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "        # Filter data between min_date and max_date\n",
    "        if min_date:\n",
    "            min_date = dateutil.parser.parse(min_date) if isinstance(min_date, str) else min_date\n",
    "            df = df[df['date'] >= min_date]\n",
    "\n",
    "        if max_date:\n",
    "            max_date = dateutil.parser.parse(max_date) if isinstance(max_date, str) else max_date\n",
    "            df = df[df['date'] <= max_date]\n",
    "\n",
    "        # Drop the date column after filtering\n",
    "        df.drop(columns=['date'], inplace=True)\n",
    "        \n",
    "    if target not in features:\n",
    "        features = [target] + features\n",
    "\n",
    "    all_columns = features\n",
    "    if not all(col in df.columns for col in all_columns):\n",
    "        missing = [col for col in all_columns if col not in df.columns]\n",
    "        raise ValueError(f\"Missing columns in dataset: {missing}\")\n",
    "\n",
    "    train_size = int(0.8 * len(df))\n",
    "    train_df = df.iloc[:train_size].copy()\n",
    "    test_df = df.iloc[train_size:].copy()\n",
    "\n",
    "    print(f\"Train size: {len(train_df)} | Test size: {len(test_df)}\")\n",
    "\n",
    "    # ------------------- Detrending Step -------------------\n",
    "    degree = 2  # Polynomial degree for detrending (can be tuned)\n",
    "    \n",
    "    train_idx = np.arange(len(train_df))\n",
    "    test_idx = np.arange(len(test_df)) + len(train_df)\n",
    "\n",
    "    # Center indices to prevent floating-point instability in polynomial fitting\n",
    "    train_idx_centered = train_idx - train_idx.mean()\n",
    "    test_idx_centered = test_idx - train_idx.mean()\n",
    "\n",
    "    # Fit polynomial trend using training data for both the target and features\n",
    "    poly_dict = {}  # Store polynomial trends for all columns\n",
    "\n",
    "    for col in features:\n",
    "        coeffs = np.polyfit(train_idx_centered, train_df[col].values, degree)\n",
    "        poly = np.poly1d(coeffs)\n",
    "        poly_dict[col] = poly\n",
    "\n",
    "        # Remove trend\n",
    "        train_df[col] -= poly(train_idx_centered)\n",
    "        test_df[col] -= poly(test_idx_centered)\n",
    "    # -------------------------------------------------------\n",
    "\n",
    "    # Normalize features using training statistics\n",
    "    train_mean, train_std = train_df[all_columns].mean(), train_df[all_columns].std()\n",
    "    train_df[all_columns] = (train_df[all_columns] - train_mean) / (train_std + 1e-8)\n",
    "    test_df[all_columns] = (test_df[all_columns] - train_mean) / (train_std + 1e-8)\n",
    "\n",
    "    # Prepare sequences for training/testing\n",
    "    X_train, y_train = df_to_X_y(train_df, features, target, window_size, horizon)\n",
    "    X_test, y_test = df_to_X_y(test_df, features, target, window_size, horizon)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    train_data = TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32).to(device),\n",
    "        torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "        )\n",
    "    test_data = TensorDataset(\n",
    "        torch.tensor(X_test, dtype=torch.float32).to(device),\n",
    "        torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "        )\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "    test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "    return train_loader, test_loader, poly_dict  # Return polynomial trends for re-trending\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- #\n",
    "#          LSTM Model         #\n",
    "# --------------------------- #\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, num_features, time_window, output_window, num_labels, num_layers=2, hidden_size=16, dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.output_window = output_window\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        self.lstm = nn.LSTM(num_features,\n",
    "                            hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True,\n",
    "                            dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, num_labels * output_window)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, D = x.shape  \n",
    "        x_, _ = self.lstm(x)\n",
    "        last_hidden = x_[:, -1, :]  # Take last time step's hidden state\n",
    "        x_ = self.fc(last_hidden)\n",
    "        x_ = x_.reshape(B, self.output_window, self.num_labels)\n",
    "        return x_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- #\n",
    "#      Training Function      #\n",
    "# --------------------------- #\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=30, lr=1e-3, wd=1e-5):\n",
    "    \"\"\"\n",
    "    Trains an LSTM model with validation.\n",
    "\n",
    "    Args:\n",
    "    - model (nn.Module): The LSTM model.\n",
    "    - train_loader (DataLoader): DataLoader for training.\n",
    "    - val_loader (DataLoader): DataLoader for validation.\n",
    "    - num_epochs (int): Number of training epochs.\n",
    "    - lr (float): Learning rate.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr,weight_decay=wd)\n",
    "    device = \"cuda\"\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # print(f\"\\nEPOCH {epoch+1}\")\n",
    "        \n",
    "        # -------------------- Training -------------------- #\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        \n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "            output = model(batch_x)\n",
    "            loss = criterion(output, batch_y.unsqueeze(-1))  # Ensure correct shape\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # -------------------- Validation -------------------- #\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient calculations\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "                output = model(batch_x)\n",
    "                loss = criterion(output, batch_y.unsqueeze(-1))  # Ensure correct shape\n",
    "                running_val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = running_val_loss / len(val_loader)\n",
    "        avg_val_loss = running_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "\n",
    "        # Print losses for the epoch\n",
    "        # print(f\"Train Loss: {avg_train_loss:.4f} | Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "    \n",
    "# --------------------------- #\n",
    "#      Evaluation Function    #\n",
    "# --------------------------- #\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"\n",
    "    Evaluates the LSTM model on test data.\n",
    "\n",
    "    Args:\n",
    "    - model (nn.Module): Trained LSTM model.\n",
    "    - test_loader (DataLoader): DataLoader for testing.\n",
    "\n",
    "    Returns:\n",
    "    - mean_mae (float): Mean Absolute Error.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    criterion = nn.L1Loss(reduction='mean')\n",
    "    mae_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x, batch_y = batch_x.to(\"cuda\"), batch_y.to(\"cuda\")\n",
    "            predictions = model(batch_x)\n",
    "            mae = criterion(predictions, batch_y.unsqueeze(-1)).item()\n",
    "            mae_list.append(mae)\n",
    "\n",
    "    return np.mean(mae_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "def run_experiment_mae_independent_lstm():\n",
    "    \"\"\"\n",
    "    Runs independent LSTM experiments for multiple datasets and horizons.\n",
    "    Appends results to output.txt with dataset names, site-wise MAEs, and horizon details.\n",
    "    \"\"\"\n",
    "    datasets = [\n",
    "        {\n",
    "            'name': 'Air Quality',\n",
    "            'directory': \"../processed_ds/air_quality_cluster/\",\n",
    "            'features': ['PM2.5', 'OT', 'PM10', 'NO2'],\n",
    "            'target': 'PM2.5',\n",
    "            'min_date': \"2014-09-01\",\n",
    "            'max_date': \"2014-11-12 19:00\",\n",
    "            'num_features': 4\n",
    "        },\n",
    "        # {\n",
    "        #     'name': 'Solar',\n",
    "        #     'directory': \"../processed_ds/solar/\",\n",
    "        #     'features': ['loc-1', 'loc-2', 'loc-3', 'loc-4'],\n",
    "        #     'target': 'loc-1',\n",
    "        #     'min_date': \"2006-09-01\",\n",
    "        #     'max_date': \"2006-09-08 4:50\",\n",
    "        #     'num_features': 4\n",
    "        # },\n",
    "        # {\n",
    "        #     'name': 'Crypto',\n",
    "        #     'directory': \"../processed_ds/crypto-data/\",\n",
    "        #     'features': ['Open', 'High', 'Low', 'OT', 'Volume'],\n",
    "        #     'target': 'OT',\n",
    "        #     'min_date': \"2018-04-01\",\n",
    "        #     'max_date': \"2018-06-15\",\n",
    "        #     'num_features': 5\n",
    "        # },\n",
    "        # {\n",
    "        #     'name': 'Sales',\n",
    "        #     'directory': \"../processed_ds/stores_data/\",\n",
    "        #     'min_date': \"2013-01-16\",\n",
    "        #     'max_date': \"2015-07-31\",\n",
    "        #     'num_features': 7\n",
    "        # }\n",
    "    ]\n",
    "\n",
    "    horizons = [1,2,4,8,16]\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    seq_len = 32\n",
    "    batch_size = 32\n",
    "    num_epochs = 30\n",
    "\n",
    "    for dataset in datasets:\n",
    "        site_files = [\n",
    "            os.path.join(dataset['directory'], f, f\"{f}.csv\")\n",
    "            for f in os.listdir(dataset['directory'])\n",
    "            if os.path.isdir(os.path.join(dataset['directory'], f))\n",
    "        ]\n",
    "\n",
    "        for horizon in horizons:\n",
    "            site_mae_list = []\n",
    "            print(f\"\\n==================== Dataset: {dataset['name']} | Horizon: {horizon} ====================\")\n",
    "\n",
    "            for site_path in site_files[0:30]:\n",
    "                print(f\"\\nProcessing Site: {site_path}\")\n",
    "\n",
    "                # Load data\n",
    "                train_loader, test_loader = load_and_preprocess_site_data(\n",
    "                    site_path,\n",
    "                    features=dataset['features'],\n",
    "                    target=dataset['target'],\n",
    "                    horizon=horizon,\n",
    "                    min_date=dataset['min_date'],\n",
    "                    max_date=dataset['max_date'],\n",
    "                    batch_size=batch_size,\n",
    "                    device=device\n",
    "                )\n",
    "\n",
    "                # LSTM Model Definition\n",
    "                model = LSTMModel(\n",
    "                    num_features=dataset['num_features'],\n",
    "                    time_window=32,\n",
    "                    output_window=horizon,\n",
    "                    num_labels=1,\n",
    "                    num_layers=2,\n",
    "                    hidden_size=16,\n",
    "                    dropout=0.5\n",
    "                )\n",
    "                total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "                print(\"Total trainable parameters:\", total_params)\n",
    "                model.to(device)\n",
    "\n",
    "                # Training setup\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "                scheduler = ExponentialLR(optimizer, gamma=0.95)\n",
    "                criterion = torch.nn.MSELoss()\n",
    "\n",
    "                # Training loop\n",
    "                model.train()\n",
    "                for epoch in range(num_epochs):\n",
    "                    epoch_train_losses = []\n",
    "                    for batch_x, batch_y in train_loader:\n",
    "                        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                        optimizer.zero_grad()\n",
    "                        output = model(batch_x)\n",
    "                        loss = criterion(output, batch_y.unsqueeze(-1))\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        epoch_train_losses.append(loss.item())\n",
    "                    scheduler.step()\n",
    "                    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {np.mean(epoch_train_losses):.4f}\")\n",
    "\n",
    "                # Evaluation\n",
    "                model.eval()\n",
    "                mae_criterion = torch.nn.L1Loss()\n",
    "                test_preds, test_targets = [], []\n",
    "                with torch.no_grad():\n",
    "                    for batch_x, batch_y in test_loader:\n",
    "                        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                        preds = model(batch_x)\n",
    "                        test_preds.append(preds.cpu())\n",
    "                        test_targets.append(batch_y.unsqueeze(-1).cpu())\n",
    "                    \n",
    "                test_preds = torch.cat(test_preds, dim=0)\n",
    "                test_targets = torch.cat(test_targets, dim=0)\n",
    "                \n",
    "            \n",
    "                test_mae = mae_criterion(test_preds, test_targets)\n",
    "                print(f\"Site: {site_path}, Test MAE: {test_mae.item():.4f}\")\n",
    "                site_mae_list.append(test_mae.item())\n",
    "\n",
    "            # Average MAE across all sites for this horizon\n",
    "            avg_mae = np.mean(site_mae_list)\n",
    "            print(f\"\\nAverage MAE for {dataset['name']} at horizon {horizon}: {avg_mae:.4f}\")\n",
    "\n",
    "            # Append results to output.txt\n",
    "            with open(\"output_test.txt\", \"a\") as f:\n",
    "                f.write(\"\\n==================== LSTM INDEPENDENT FORECASTING MODEL RESULTS ====================\\n\")\n",
    "                f.write(f\"Dataset: {dataset['name']}\\n\")\n",
    "                f.write(f\"Horizon: {horizon}\\n\")\n",
    "                f.write(f\"MAE per site: {site_mae_list}\\n\")\n",
    "                f.write(f\"Mean MAE: {avg_mae:.4f}\\n\")\n",
    "            # TODO: FIX THIS before storing results.\n",
    "            # store_results(\n",
    "            #     dataset_name=dataset['name'],\n",
    "            #     horizons=[1,2,4,8,16],\n",
    "            #     experiment_type='independent',\n",
    "            #     mae_result=site_mae_list,\n",
    "            #     backbone='simple_lstm',\n",
    "            #     horizon_value=horizon\n",
    "            # )\n",
    "\n",
    "    print(\"\\n🏆 All independent LSTM Forecasting Model experiments are complete! 🏆\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "def run_experiment_mae_independent_lstm_detrend():\n",
    "    \"\"\"\n",
    "    Runs independent LSTM experiments for multiple datasets and horizons.\n",
    "    Appends results to output.txt with dataset names, site-wise MAEs, and horizon details.\n",
    "    \"\"\"\n",
    "    datasets = [\n",
    "        # {\n",
    "        #     'name': 'Air Quality',\n",
    "        #     'directory': \"../processed_ds/air_quality_cluster/\",\n",
    "        #     'features': ['PM2.5', 'OT', 'PM10', 'NO2'],\n",
    "        #     'target': 'PM2.5',\n",
    "        #     'min_date': \"2014-09-01\",\n",
    "        #     'max_date': \"2014-11-12 19:00\",\n",
    "        #     'num_features': 4\n",
    "        # },\n",
    "        {\n",
    "            'name': 'Solar',\n",
    "            'directory': \"../processed_ds/solar/\",\n",
    "            'features': ['loc-1', 'loc-2', 'loc-3', 'loc-4'],\n",
    "            'target': 'loc-1',\n",
    "            'min_date': \"2006-09-01\",\n",
    "            'max_date': \"2006-09-08 4:50\",\n",
    "            'num_features': 4\n",
    "        },\n",
    "        # {\n",
    "        #     'name': 'Crypto',\n",
    "        #     'directory': \"../processed_ds/crypto-data/\",\n",
    "        #     'features': ['Open', 'High', 'Low', 'OT', 'Volume'],\n",
    "        #     'target': 'OT',\n",
    "        #     'min_date': \"2018-04-01\",\n",
    "        #     'max_date': \"2018-06-15\",\n",
    "        #     'num_features': 5\n",
    "        # },\n",
    "        # {\n",
    "        #     'name': 'Sales',\n",
    "        #     'directory': \"../processed_ds/stores_data/\",\n",
    "        #     'min_date': \"2013-01-16\",\n",
    "        #     'max_date': \"2015-07-31\",\n",
    "        #     'num_features': 7\n",
    "        # }\n",
    "    ]\n",
    "\n",
    "    horizons = [1,2,4,8,16]\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    seq_len = 32\n",
    "    batch_size = 16\n",
    "    num_epochs = 40\n",
    "\n",
    "    for dataset in datasets:\n",
    "        site_files = [\n",
    "            os.path.join(dataset['directory'], f, f\"{f}.csv\")\n",
    "            for f in os.listdir(dataset['directory'])\n",
    "            if os.path.isdir(os.path.join(dataset['directory'], f))\n",
    "        ]\n",
    "\n",
    "        for horizon in horizons:\n",
    "            site_mae_list = []\n",
    "            print(f\"\\n==================== Dataset: {dataset['name']} | Horizon: {horizon} ====================\")\n",
    "\n",
    "            for site_path in site_files[0:30]:\n",
    "                print(f\"\\nProcessing Site: {site_path}\")\n",
    "\n",
    "                # Load data\n",
    "                train_loader, test_loader, poly_dict = load_and_preprocess_site_data_detrend(\n",
    "                    site_path,\n",
    "                    features=dataset['features'],\n",
    "                    target=dataset['target'],\n",
    "                    horizon=horizon,\n",
    "                    min_date=dataset['min_date'],\n",
    "                    max_date=dataset['max_date'],\n",
    "                    batch_size=batch_size,\n",
    "                    device=device\n",
    "                )\n",
    "\n",
    "                # LSTM Model Definition\n",
    "                model = LSTMModel(\n",
    "                    num_features=dataset['num_features'],\n",
    "                    time_window=32,\n",
    "                    output_window=horizon,\n",
    "                    num_labels=1,\n",
    "                    num_layers=2,\n",
    "                    hidden_size=16,\n",
    "                    dropout=0.5\n",
    "                )\n",
    "                total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "                print(\"Total trainable parameters:\", total_params)\n",
    "                model.to(device)\n",
    "\n",
    "                # Training setup\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "                scheduler = ExponentialLR(optimizer, gamma=0.95)\n",
    "                criterion = torch.nn.MSELoss()\n",
    "\n",
    "                # Training loop\n",
    "                model.train()\n",
    "                for epoch in range(num_epochs):\n",
    "                    epoch_train_losses = []\n",
    "                    for batch_x, batch_y in train_loader:\n",
    "                        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                        optimizer.zero_grad()\n",
    "                        output = model(batch_x)\n",
    "                        loss = criterion(output, batch_y.unsqueeze(-1))\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        epoch_train_losses.append(loss.item())\n",
    "                    scheduler.step()\n",
    "                    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {np.mean(epoch_train_losses):.4f}\")\n",
    "\n",
    "                # Evaluation\n",
    "                model.eval()\n",
    "                mae_criterion = torch.nn.L1Loss()\n",
    "                test_preds, test_targets, test_indices, test_features = [], [], [], []\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for i, (batch_x, batch_y) in enumerate(test_loader):\n",
    "                        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                        preds = model(batch_x)\n",
    "                        \n",
    "                        test_preds.append(preds.cpu())\n",
    "                        test_targets.append(batch_y.unsqueeze(-1).cpu())\n",
    "                        test_features.append(batch_x.cpu())\n",
    "\n",
    "                        # Store test indices for inverse detrending\n",
    "                        start_idx = i * batch_size\n",
    "                        end_idx = start_idx + len(batch_x)\n",
    "                        test_indices.extend(range(start_idx, end_idx))\n",
    "\n",
    "                # Convert lists to tensors and remove extra dimensions\n",
    "                test_preds = torch.cat(test_preds, dim=0).squeeze().numpy()  # Shape: (N, horizon)\n",
    "                test_targets = torch.cat(test_targets, dim=0).squeeze().numpy()  # Shape: (N, horizon)\n",
    "                test_features = torch.cat(test_features, dim=0).numpy()  # Shape: (N, seq_len, num_features)\n",
    "                \n",
    "                if test_preds.ndim == 1:\n",
    "                    test_preds = np.expand_dims(test_preds, axis=1)  # Shape: (N, 1)\n",
    "                if test_targets.ndim == 1:\n",
    "                    test_targets = np.expand_dims(test_targets, axis=1)  # Shape: (N, 1)\n",
    "                \n",
    "                # ✅ Ensure test_indices has correct length\n",
    "                test_indices = np.array(test_indices[:len(test_preds)])\n",
    "\n",
    "                # ✅ **Apply inverse detrending (Add trend back to predictions, targets, and features)**\n",
    "                for col_idx, col_name in enumerate(['loc-1', 'loc-2', 'loc-3', 'loc-4']):\n",
    "                    trend_values = poly_dict[col_name](test_indices)  # Shape: (N,)\n",
    "                    if col_name == 'loc-1':\n",
    "                        # Expand trend across forecast horizons for target\n",
    "                        trend_values = np.expand_dims(trend_values, axis=1)  # Shape: (N, 1)\n",
    "                        if test_preds.shape[1] > 1:  # Only tile if horizon > 1\n",
    "                            trend_values = np.tile(trend_values, (1, test_preds.shape[1]))  # Expand across horizon\n",
    "                        test_preds += trend_values\n",
    "                        test_targets += trend_values\n",
    "                    else:\n",
    "                        # Apply to each feature in test_features\n",
    "                        # test_features[:, :, col_idx] += np.expand_dims(trend_values, axis=1)  # Shape: (N, seq_len)\n",
    "                        # Apply to each feature in test_features\n",
    "                        trend_values = np.expand_dims(trend_values, axis=1)  # Shape: (N, 1)\n",
    "                        trend_values = np.tile(trend_values, (1, test_features.shape[1]))  # Expand across seq_len\n",
    "                        test_features[:, :, col_idx] += trend_values  # Add trend back to each feature\n",
    "                        \n",
    "                # ✅ Convert back to PyTorch tensors for MAE calculation\n",
    "                test_preds = torch.tensor(test_preds, dtype=torch.float32)\n",
    "                test_targets = torch.tensor(test_targets, dtype=torch.float32)\n",
    "\n",
    "                # ✅ Compute MAE\n",
    "                test_mae = mae_criterion(test_preds, test_targets)\n",
    "                print(f\"Site: {site_path}, Test MAE: {test_mae.item():.4f}\")\n",
    "                site_mae_list.append(test_mae.item())\n",
    "\n",
    "            # Average MAE across all sites for this horizon\n",
    "            avg_mae = np.mean(site_mae_list)\n",
    "            print(f\"\\nAverage MAE for {dataset['name']} at horizon {horizon}: {avg_mae:.4f}\")\n",
    "\n",
    "            # Append results to output.txt\n",
    "            with open(\"output_test.txt\", \"a\") as f:\n",
    "                f.write(\"\\n==================== LSTM INDEPENDENT FORECASTING MODEL RESULTS ====================\\n\")\n",
    "                f.write(f\"Dataset: {dataset['name']}\\n\")\n",
    "                f.write(f\"Horizon: {horizon}\\n\")\n",
    "                f.write(f\"MAE per site: {site_mae_list}\\n\")\n",
    "                f.write(f\"Mean MAE: {avg_mae:.4f}\\n\")\n",
    "\n",
    "    print(\"\\n🏆 All independent LSTM Forecasting Model experiments are complete! 🏆\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Dataset: Air Quality | Horizon: 1 ====================\n",
      "\n",
      "Processing Site: ../processed_ds/air_quality_cluster/site-11/site-11.csv\n",
      "Train size: 1398 | Validation size: 0 | Test size: 350\n",
      "Total trainable parameters: 3601\n",
      "Epoch 1/30 - Train Loss: 1.0192\n",
      "Epoch 2/30 - Train Loss: 0.9972\n",
      "Epoch 3/30 - Train Loss: 0.9709\n",
      "Epoch 4/30 - Train Loss: 0.9251\n",
      "Epoch 5/30 - Train Loss: 0.9072\n",
      "Epoch 6/30 - Train Loss: 0.8555\n",
      "Epoch 7/30 - Train Loss: 0.7962\n",
      "Epoch 8/30 - Train Loss: 0.7270\n",
      "Epoch 9/30 - Train Loss: 0.6570\n",
      "Epoch 10/30 - Train Loss: 0.5977\n",
      "Epoch 11/30 - Train Loss: 0.5314\n",
      "Epoch 12/30 - Train Loss: 0.4617\n",
      "Epoch 13/30 - Train Loss: 0.4261\n",
      "Epoch 14/30 - Train Loss: 0.3909\n",
      "Epoch 15/30 - Train Loss: 0.3561\n",
      "Epoch 16/30 - Train Loss: 0.3391\n",
      "Epoch 17/30 - Train Loss: 0.3215\n",
      "Epoch 18/30 - Train Loss: 0.3126\n",
      "Epoch 19/30 - Train Loss: 0.3023\n",
      "Epoch 20/30 - Train Loss: 0.2964\n",
      "Epoch 21/30 - Train Loss: 0.2888\n",
      "Epoch 22/30 - Train Loss: 0.2819\n",
      "Epoch 23/30 - Train Loss: 0.2734\n",
      "Epoch 24/30 - Train Loss: 0.2755\n",
      "Epoch 25/30 - Train Loss: 0.2696\n",
      "Epoch 26/30 - Train Loss: 0.2668\n",
      "Epoch 27/30 - Train Loss: 0.2622\n",
      "Epoch 28/30 - Train Loss: 0.2656\n",
      "Epoch 29/30 - Train Loss: 0.2597\n",
      "Epoch 30/30 - Train Loss: 0.2527\n",
      "Site: ../processed_ds/air_quality_cluster/site-11/site-11.csv, Test MAE: 0.2946\n",
      "\n",
      "Processing Site: ../processed_ds/air_quality_cluster/site-4/site-4.csv\n",
      "Train size: 1398 | Validation size: 0 | Test size: 350\n",
      "Total trainable parameters: 3601\n",
      "Epoch 1/30 - Train Loss: 1.0590\n",
      "Epoch 2/30 - Train Loss: 1.0383\n",
      "Epoch 3/30 - Train Loss: 1.0064\n",
      "Epoch 4/30 - Train Loss: 0.9739\n",
      "Epoch 5/30 - Train Loss: 0.9340\n",
      "Epoch 6/30 - Train Loss: 0.8778\n",
      "Epoch 7/30 - Train Loss: 0.8171\n",
      "Epoch 8/30 - Train Loss: 0.7636\n",
      "Epoch 9/30 - Train Loss: 0.6887\n",
      "Epoch 10/30 - Train Loss: 0.6100\n",
      "Epoch 11/30 - Train Loss: 0.5474\n",
      "Epoch 12/30 - Train Loss: 0.4907\n",
      "Epoch 13/30 - Train Loss: 0.4448\n",
      "Epoch 14/30 - Train Loss: 0.4169\n",
      "Epoch 15/30 - Train Loss: 0.3855\n",
      "Epoch 16/30 - Train Loss: 0.3623\n",
      "Epoch 17/30 - Train Loss: 0.3532\n",
      "Epoch 18/30 - Train Loss: 0.3340\n",
      "Epoch 19/30 - Train Loss: 0.3357\n",
      "Epoch 20/30 - Train Loss: 0.3229\n",
      "Epoch 21/30 - Train Loss: 0.3163\n",
      "Epoch 22/30 - Train Loss: 0.3111\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[135], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_experiment_mae_independent_lstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[119], line 106\u001b[0m, in \u001b[0;36mrun_experiment_mae_independent_lstm\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m batch_x, batch_y \u001b[38;5;241m=\u001b[39m batch_x\u001b[38;5;241m.\u001b[39mto(device), batch_y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    105\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 106\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, batch_y\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    108\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Documents/Arnob/centralized-baseline/centralized/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Arnob/centralized-baseline/centralized/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[117], line 21\u001b[0m, in \u001b[0;36mLSTMModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     20\u001b[0m     B, T, D \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape  \n\u001b[0;32m---> 21\u001b[0m     x_, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     last_hidden \u001b[38;5;241m=\u001b[39m x_[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# Take last time step's hidden state\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     x_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(last_hidden)\n",
      "File \u001b[0;32m~/Documents/Arnob/centralized-baseline/centralized/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Arnob/centralized-baseline/centralized/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Arnob/centralized-baseline/centralized/lib/python3.8/site-packages/torch/nn/modules/rnn.py:917\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    914\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 917\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    920\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    921\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_experiment_mae_independent_lstm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Dataset: Solar | Horizon: 1 ====================\n",
      "\n",
      "Processing Site: ../processed_ds/solar/solar_al/solar_al.csv\n",
      "Train size: 1660 | Test size: 415\n",
      "Total trainable parameters: 3601\n",
      "Epoch 1/40 - Train Loss: 0.9817\n",
      "Epoch 2/40 - Train Loss: 0.9180\n",
      "Epoch 3/40 - Train Loss: 0.8030\n",
      "Epoch 4/40 - Train Loss: 0.5795\n",
      "Epoch 5/40 - Train Loss: 0.3672\n",
      "Epoch 6/40 - Train Loss: 0.2810\n",
      "Epoch 7/40 - Train Loss: 0.2506\n",
      "Epoch 8/40 - Train Loss: 0.2419\n",
      "Epoch 9/40 - Train Loss: 0.2316\n",
      "Epoch 10/40 - Train Loss: 0.2259\n",
      "Epoch 11/40 - Train Loss: 0.2218\n",
      "Epoch 12/40 - Train Loss: 0.2153\n",
      "Epoch 13/40 - Train Loss: 0.2152\n",
      "Epoch 14/40 - Train Loss: 0.2064\n",
      "Epoch 15/40 - Train Loss: 0.2083\n",
      "Epoch 16/40 - Train Loss: 0.2052\n",
      "Epoch 17/40 - Train Loss: 0.1982\n",
      "Epoch 18/40 - Train Loss: 0.1921\n",
      "Epoch 19/40 - Train Loss: 0.1899\n",
      "Epoch 20/40 - Train Loss: 0.1919\n",
      "Epoch 21/40 - Train Loss: 0.1893\n",
      "Epoch 22/40 - Train Loss: 0.1838\n",
      "Epoch 23/40 - Train Loss: 0.1832\n",
      "Epoch 24/40 - Train Loss: 0.1841\n",
      "Epoch 25/40 - Train Loss: 0.1833\n",
      "Epoch 26/40 - Train Loss: 0.1802\n",
      "Epoch 27/40 - Train Loss: 0.1769\n",
      "Epoch 28/40 - Train Loss: 0.1794\n",
      "Epoch 29/40 - Train Loss: 0.1749\n",
      "Epoch 30/40 - Train Loss: 0.1759\n",
      "Epoch 31/40 - Train Loss: 0.1719\n",
      "Epoch 32/40 - Train Loss: 0.1717\n",
      "Epoch 33/40 - Train Loss: 0.1729\n",
      "Epoch 34/40 - Train Loss: 0.1717\n",
      "Epoch 35/40 - Train Loss: 0.1729\n",
      "Epoch 36/40 - Train Loss: 0.1691\n",
      "Epoch 37/40 - Train Loss: 0.1651\n",
      "Epoch 38/40 - Train Loss: 0.1706\n",
      "Epoch 39/40 - Train Loss: 0.1657\n",
      "Epoch 40/40 - Train Loss: 0.1669\n",
      "Site: ../processed_ds/solar/solar_al/solar_al.csv, Test MAE: 0.2743\n",
      "\n",
      "Processing Site: ../processed_ds/solar/solar_il/solar_il.csv\n",
      "Train size: 1660 | Test size: 415\n",
      "Total trainable parameters: 3601\n",
      "Epoch 1/40 - Train Loss: 1.1108\n",
      "Epoch 2/40 - Train Loss: 1.0414\n",
      "Epoch 3/40 - Train Loss: 0.9688\n",
      "Epoch 4/40 - Train Loss: 0.8402\n",
      "Epoch 5/40 - Train Loss: 0.6628\n",
      "Epoch 6/40 - Train Loss: 0.4418\n",
      "Epoch 7/40 - Train Loss: 0.3208\n",
      "Epoch 8/40 - Train Loss: 0.2761\n",
      "Epoch 9/40 - Train Loss: 0.2504\n",
      "Epoch 10/40 - Train Loss: 0.2272\n",
      "Epoch 11/40 - Train Loss: 0.2153\n",
      "Epoch 12/40 - Train Loss: 0.2013\n",
      "Epoch 13/40 - Train Loss: 0.1930\n",
      "Epoch 14/40 - Train Loss: 0.1788\n",
      "Epoch 15/40 - Train Loss: 0.1702\n",
      "Epoch 16/40 - Train Loss: 0.1598\n",
      "Epoch 17/40 - Train Loss: 0.1594\n",
      "Epoch 18/40 - Train Loss: 0.1481\n",
      "Epoch 19/40 - Train Loss: 0.1446\n",
      "Epoch 20/40 - Train Loss: 0.1453\n",
      "Epoch 21/40 - Train Loss: 0.1368\n",
      "Epoch 22/40 - Train Loss: 0.1380\n",
      "Epoch 23/40 - Train Loss: 0.1319\n",
      "Epoch 24/40 - Train Loss: 0.1280\n",
      "Epoch 25/40 - Train Loss: 0.1306\n",
      "Epoch 26/40 - Train Loss: 0.1200\n",
      "Epoch 27/40 - Train Loss: 0.1235\n",
      "Epoch 28/40 - Train Loss: 0.1212\n",
      "Epoch 29/40 - Train Loss: 0.1174\n",
      "Epoch 30/40 - Train Loss: 0.1179\n",
      "Epoch 31/40 - Train Loss: 0.1127\n",
      "Epoch 32/40 - Train Loss: 0.1164\n",
      "Epoch 33/40 - Train Loss: 0.1177\n",
      "Epoch 34/40 - Train Loss: 0.1122\n",
      "Epoch 35/40 - Train Loss: 0.1085\n",
      "Epoch 36/40 - Train Loss: 0.1136\n",
      "Epoch 37/40 - Train Loss: 0.1137\n",
      "Epoch 38/40 - Train Loss: 0.1128\n",
      "Epoch 39/40 - Train Loss: 0.1081\n",
      "Epoch 40/40 - Train Loss: 0.1100\n",
      "Site: ../processed_ds/solar/solar_il/solar_il.csv, Test MAE: 0.3873\n",
      "\n",
      "Processing Site: ../processed_ds/solar/solar_me/solar_me.csv\n",
      "Train size: 1660 | Test size: 415\n",
      "Total trainable parameters: 3601\n",
      "Epoch 1/40 - Train Loss: 0.9113\n",
      "Epoch 2/40 - Train Loss: 0.7882\n",
      "Epoch 3/40 - Train Loss: 0.6174\n",
      "Epoch 4/40 - Train Loss: 0.4317\n",
      "Epoch 5/40 - Train Loss: 0.3362\n",
      "Epoch 6/40 - Train Loss: 0.2912\n",
      "Epoch 7/40 - Train Loss: 0.2727\n",
      "Epoch 8/40 - Train Loss: 0.2570\n",
      "Epoch 9/40 - Train Loss: 0.2482\n",
      "Epoch 10/40 - Train Loss: 0.2365\n",
      "Epoch 11/40 - Train Loss: 0.2313\n",
      "Epoch 12/40 - Train Loss: 0.2162\n",
      "Epoch 13/40 - Train Loss: 0.2181\n",
      "Epoch 14/40 - Train Loss: 0.2120\n",
      "Epoch 15/40 - Train Loss: 0.2028\n",
      "Epoch 16/40 - Train Loss: 0.2012\n",
      "Epoch 17/40 - Train Loss: 0.1970\n",
      "Epoch 18/40 - Train Loss: 0.1886\n",
      "Epoch 19/40 - Train Loss: 0.1962\n",
      "Epoch 20/40 - Train Loss: 0.1848\n",
      "Epoch 21/40 - Train Loss: 0.1830\n",
      "Epoch 22/40 - Train Loss: 0.1801\n",
      "Epoch 23/40 - Train Loss: 0.1821\n",
      "Epoch 24/40 - Train Loss: 0.1745\n",
      "Epoch 25/40 - Train Loss: 0.1776\n",
      "Epoch 26/40 - Train Loss: 0.1737\n",
      "Epoch 27/40 - Train Loss: 0.1745\n",
      "Epoch 28/40 - Train Loss: 0.1746\n",
      "Epoch 29/40 - Train Loss: 0.1704\n",
      "Epoch 30/40 - Train Loss: 0.1730\n",
      "Epoch 31/40 - Train Loss: 0.1690\n",
      "Epoch 32/40 - Train Loss: 0.1642\n",
      "Epoch 33/40 - Train Loss: 0.1656\n",
      "Epoch 34/40 - Train Loss: 0.1639\n",
      "Epoch 35/40 - Train Loss: 0.1715\n",
      "Epoch 36/40 - Train Loss: 0.1587\n",
      "Epoch 37/40 - Train Loss: 0.1688\n",
      "Epoch 38/40 - Train Loss: 0.1618\n",
      "Epoch 39/40 - Train Loss: 0.1637\n",
      "Epoch 40/40 - Train Loss: 0.1637\n",
      "Site: ../processed_ds/solar/solar_me/solar_me.csv, Test MAE: 0.4375\n",
      "\n",
      "Processing Site: ../processed_ds/solar/solar_ma/solar_ma.csv\n",
      "Train size: 1660 | Test size: 415\n",
      "Total trainable parameters: 3601\n",
      "Epoch 1/40 - Train Loss: 0.9937\n",
      "Epoch 2/40 - Train Loss: 0.9539\n",
      "Epoch 3/40 - Train Loss: 0.8730\n",
      "Epoch 4/40 - Train Loss: 0.7056\n",
      "Epoch 5/40 - Train Loss: 0.5093\n",
      "Epoch 6/40 - Train Loss: 0.3688\n",
      "Epoch 7/40 - Train Loss: 0.3067\n",
      "Epoch 8/40 - Train Loss: 0.2665\n",
      "Epoch 9/40 - Train Loss: 0.2347\n",
      "Epoch 10/40 - Train Loss: 0.2200\n",
      "Epoch 11/40 - Train Loss: 0.2049\n",
      "Epoch 12/40 - Train Loss: 0.1921\n",
      "Epoch 13/40 - Train Loss: 0.1826\n",
      "Epoch 14/40 - Train Loss: 0.1708\n",
      "Epoch 15/40 - Train Loss: 0.1641\n",
      "Epoch 16/40 - Train Loss: 0.1590\n",
      "Epoch 17/40 - Train Loss: 0.1533\n",
      "Epoch 18/40 - Train Loss: 0.1518\n",
      "Epoch 19/40 - Train Loss: 0.1449\n",
      "Epoch 20/40 - Train Loss: 0.1421\n",
      "Epoch 21/40 - Train Loss: 0.1384\n",
      "Epoch 22/40 - Train Loss: 0.1370\n",
      "Epoch 23/40 - Train Loss: 0.1330\n",
      "Epoch 24/40 - Train Loss: 0.1304\n",
      "Epoch 25/40 - Train Loss: 0.1275\n",
      "Epoch 26/40 - Train Loss: 0.1237\n",
      "Epoch 27/40 - Train Loss: 0.1276\n",
      "Epoch 28/40 - Train Loss: 0.1249\n",
      "Epoch 29/40 - Train Loss: 0.1220\n",
      "Epoch 30/40 - Train Loss: 0.1244\n",
      "Epoch 31/40 - Train Loss: 0.1181\n",
      "Epoch 32/40 - Train Loss: 0.1163\n",
      "Epoch 33/40 - Train Loss: 0.1190\n",
      "Epoch 34/40 - Train Loss: 0.1160\n",
      "Epoch 35/40 - Train Loss: 0.1145\n",
      "Epoch 36/40 - Train Loss: 0.1163\n",
      "Epoch 37/40 - Train Loss: 0.1124\n",
      "Epoch 38/40 - Train Loss: 0.1165\n",
      "Epoch 39/40 - Train Loss: 0.1144\n",
      "Epoch 40/40 - Train Loss: 0.1153\n",
      "Site: ../processed_ds/solar/solar_ma/solar_ma.csv, Test MAE: 0.6261\n",
      "\n",
      "Processing Site: ../processed_ds/solar/solar_ks/solar_ks.csv\n",
      "Train size: 1660 | Test size: 415\n",
      "Total trainable parameters: 3601\n",
      "Epoch 1/40 - Train Loss: 0.9773\n",
      "Epoch 2/40 - Train Loss: 0.8455\n",
      "Epoch 3/40 - Train Loss: 0.6148\n",
      "Epoch 4/40 - Train Loss: 0.3703\n",
      "Epoch 5/40 - Train Loss: 0.2600\n",
      "Epoch 6/40 - Train Loss: 0.2263\n",
      "Epoch 7/40 - Train Loss: 0.2110\n",
      "Epoch 8/40 - Train Loss: 0.1968\n",
      "Epoch 9/40 - Train Loss: 0.1964\n",
      "Epoch 10/40 - Train Loss: 0.1893\n",
      "Epoch 11/40 - Train Loss: 0.1881\n",
      "Epoch 12/40 - Train Loss: 0.1830\n",
      "Epoch 13/40 - Train Loss: 0.1758\n",
      "Epoch 14/40 - Train Loss: 0.1731\n",
      "Epoch 15/40 - Train Loss: 0.1687\n",
      "Epoch 16/40 - Train Loss: 0.1637\n",
      "Epoch 17/40 - Train Loss: 0.1626\n",
      "Epoch 18/40 - Train Loss: 0.1620\n",
      "Epoch 19/40 - Train Loss: 0.1563\n",
      "Epoch 20/40 - Train Loss: 0.1560\n",
      "Epoch 21/40 - Train Loss: 0.1543\n",
      "Epoch 22/40 - Train Loss: 0.1479\n",
      "Epoch 23/40 - Train Loss: 0.1544\n",
      "Epoch 24/40 - Train Loss: 0.1471\n",
      "Epoch 25/40 - Train Loss: 0.1449\n",
      "Epoch 26/40 - Train Loss: 0.1453\n",
      "Epoch 27/40 - Train Loss: 0.1395\n",
      "Epoch 28/40 - Train Loss: 0.1371\n",
      "Epoch 29/40 - Train Loss: 0.1374\n",
      "Epoch 30/40 - Train Loss: 0.1408\n",
      "Epoch 31/40 - Train Loss: 0.1320\n",
      "Epoch 32/40 - Train Loss: 0.1358\n",
      "Epoch 33/40 - Train Loss: 0.1317\n",
      "Epoch 34/40 - Train Loss: 0.1296\n",
      "Epoch 35/40 - Train Loss: 0.1299\n",
      "Epoch 36/40 - Train Loss: 0.1251\n",
      "Epoch 37/40 - Train Loss: 0.1240\n",
      "Epoch 38/40 - Train Loss: 0.1230\n",
      "Epoch 39/40 - Train Loss: 0.1215\n",
      "Epoch 40/40 - Train Loss: 0.1243\n",
      "Site: ../processed_ds/solar/solar_ks/solar_ks.csv, Test MAE: 0.9756\n",
      "\n",
      "Processing Site: ../processed_ds/solar/solar_fl/solar_fl.csv\n",
      "Train size: 1660 | Test size: 415\n",
      "Total trainable parameters: 3601\n",
      "Epoch 1/40 - Train Loss: 0.9722\n",
      "Epoch 2/40 - Train Loss: 0.8616\n",
      "Epoch 3/40 - Train Loss: 0.6921\n",
      "Epoch 4/40 - Train Loss: 0.5142\n",
      "Epoch 5/40 - Train Loss: 0.3922\n",
      "Epoch 6/40 - Train Loss: 0.3194\n",
      "Epoch 7/40 - Train Loss: 0.2736\n",
      "Epoch 8/40 - Train Loss: 0.2262\n",
      "Epoch 9/40 - Train Loss: 0.1982\n",
      "Epoch 10/40 - Train Loss: 0.1794\n",
      "Epoch 11/40 - Train Loss: 0.1661\n",
      "Epoch 12/40 - Train Loss: 0.1557\n",
      "Epoch 13/40 - Train Loss: 0.1537\n",
      "Epoch 14/40 - Train Loss: 0.1524\n",
      "Epoch 15/40 - Train Loss: 0.1409\n",
      "Epoch 16/40 - Train Loss: 0.1396\n",
      "Epoch 17/40 - Train Loss: 0.1402\n",
      "Epoch 18/40 - Train Loss: 0.1388\n",
      "Epoch 19/40 - Train Loss: 0.1378\n",
      "Epoch 20/40 - Train Loss: 0.1318\n",
      "Epoch 21/40 - Train Loss: 0.1320\n",
      "Epoch 22/40 - Train Loss: 0.1288\n",
      "Epoch 23/40 - Train Loss: 0.1299\n",
      "Epoch 24/40 - Train Loss: 0.1292\n",
      "Epoch 25/40 - Train Loss: 0.1286\n",
      "Epoch 26/40 - Train Loss: 0.1259\n",
      "Epoch 27/40 - Train Loss: 0.1262\n",
      "Epoch 28/40 - Train Loss: 0.1231\n",
      "Epoch 29/40 - Train Loss: 0.1205\n",
      "Epoch 30/40 - Train Loss: 0.1218\n",
      "Epoch 31/40 - Train Loss: 0.1221\n",
      "Epoch 32/40 - Train Loss: 0.1260\n",
      "Epoch 33/40 - Train Loss: 0.1217\n",
      "Epoch 34/40 - Train Loss: 0.1219\n",
      "Epoch 35/40 - Train Loss: 0.1231\n",
      "Epoch 36/40 - Train Loss: 0.1212\n",
      "Epoch 37/40 - Train Loss: 0.1217\n",
      "Epoch 38/40 - Train Loss: 0.1175\n",
      "Epoch 39/40 - Train Loss: 0.1191\n",
      "Epoch 40/40 - Train Loss: 0.1188\n",
      "Site: ../processed_ds/solar/solar_fl/solar_fl.csv, Test MAE: 0.4279\n",
      "\n",
      "Average MAE for Solar at horizon 1: 0.5214\n",
      "\n",
      "==================== Dataset: Solar | Horizon: 2 ====================\n",
      "\n",
      "Processing Site: ../processed_ds/solar/solar_al/solar_al.csv\n",
      "Train size: 1660 | Test size: 415\n",
      "Total trainable parameters: 3618\n",
      "Epoch 1/40 - Train Loss: 1.0216\n",
      "Epoch 2/40 - Train Loss: 0.9566\n",
      "Epoch 3/40 - Train Loss: 0.8592\n",
      "Epoch 4/40 - Train Loss: 0.7013\n",
      "Epoch 5/40 - Train Loss: 0.5068\n",
      "Epoch 6/40 - Train Loss: 0.3673\n",
      "Epoch 7/40 - Train Loss: 0.3009\n",
      "Epoch 8/40 - Train Loss: 0.2682\n",
      "Epoch 9/40 - Train Loss: 0.2511\n",
      "Epoch 10/40 - Train Loss: 0.2371\n",
      "Epoch 11/40 - Train Loss: 0.2259\n",
      "Epoch 12/40 - Train Loss: 0.2201\n",
      "Epoch 13/40 - Train Loss: 0.2158\n",
      "Epoch 14/40 - Train Loss: 0.2102\n",
      "Epoch 15/40 - Train Loss: 0.2059\n",
      "Epoch 16/40 - Train Loss: 0.1980\n",
      "Epoch 17/40 - Train Loss: 0.1957\n",
      "Epoch 18/40 - Train Loss: 0.1934\n",
      "Epoch 19/40 - Train Loss: 0.1874\n",
      "Epoch 20/40 - Train Loss: 0.1872\n",
      "Epoch 21/40 - Train Loss: 0.1855\n",
      "Epoch 22/40 - Train Loss: 0.1817\n",
      "Epoch 23/40 - Train Loss: 0.1783\n",
      "Epoch 24/40 - Train Loss: 0.1781\n",
      "Epoch 25/40 - Train Loss: 0.1794\n",
      "Epoch 26/40 - Train Loss: 0.1756\n",
      "Epoch 27/40 - Train Loss: 0.1757\n",
      "Epoch 28/40 - Train Loss: 0.1691\n",
      "Epoch 29/40 - Train Loss: 0.1715\n",
      "Epoch 30/40 - Train Loss: 0.1693\n",
      "Epoch 31/40 - Train Loss: 0.1671\n",
      "Epoch 32/40 - Train Loss: 0.1661\n",
      "Epoch 33/40 - Train Loss: 0.1686\n",
      "Epoch 34/40 - Train Loss: 0.1655\n",
      "Epoch 35/40 - Train Loss: 0.1670\n",
      "Epoch 36/40 - Train Loss: 0.1602\n",
      "Epoch 37/40 - Train Loss: 0.1651\n",
      "Epoch 38/40 - Train Loss: 0.1593\n",
      "Epoch 39/40 - Train Loss: 0.1586\n",
      "Epoch 40/40 - Train Loss: 0.1640\n",
      "Site: ../processed_ds/solar/solar_al/solar_al.csv, Test MAE: 0.3249\n",
      "\n",
      "Processing Site: ../processed_ds/solar/solar_il/solar_il.csv\n",
      "Train size: 1660 | Test size: 415\n",
      "Total trainable parameters: 3618\n",
      "Epoch 1/40 - Train Loss: 1.0056\n",
      "Epoch 2/40 - Train Loss: 0.9279\n",
      "Epoch 3/40 - Train Loss: 0.7915\n",
      "Epoch 4/40 - Train Loss: 0.5951\n",
      "Epoch 5/40 - Train Loss: 0.4459\n",
      "Epoch 6/40 - Train Loss: 0.3644\n",
      "Epoch 7/40 - Train Loss: 0.3240\n",
      "Epoch 8/40 - Train Loss: 0.2979\n",
      "Epoch 9/40 - Train Loss: 0.2781\n",
      "Epoch 10/40 - Train Loss: 0.2595\n",
      "Epoch 11/40 - Train Loss: 0.2441\n",
      "Epoch 12/40 - Train Loss: 0.2364\n",
      "Epoch 13/40 - Train Loss: 0.2177\n",
      "Epoch 14/40 - Train Loss: 0.2083\n",
      "Epoch 15/40 - Train Loss: 0.1967\n",
      "Epoch 16/40 - Train Loss: 0.1917\n",
      "Epoch 17/40 - Train Loss: 0.1840\n",
      "Epoch 18/40 - Train Loss: 0.1739\n",
      "Epoch 19/40 - Train Loss: 0.1692\n",
      "Epoch 20/40 - Train Loss: 0.1602\n",
      "Epoch 21/40 - Train Loss: 0.1607\n",
      "Epoch 22/40 - Train Loss: 0.1511\n",
      "Epoch 23/40 - Train Loss: 0.1484\n",
      "Epoch 24/40 - Train Loss: 0.1440\n",
      "Epoch 25/40 - Train Loss: 0.1427\n",
      "Epoch 26/40 - Train Loss: 0.1383\n",
      "Epoch 27/40 - Train Loss: 0.1389\n",
      "Epoch 28/40 - Train Loss: 0.1366\n",
      "Epoch 29/40 - Train Loss: 0.1294\n",
      "Epoch 30/40 - Train Loss: 0.1311\n",
      "Epoch 31/40 - Train Loss: 0.1283\n",
      "Epoch 32/40 - Train Loss: 0.1287\n",
      "Epoch 33/40 - Train Loss: 0.1262\n",
      "Epoch 34/40 - Train Loss: 0.1247\n",
      "Epoch 35/40 - Train Loss: 0.1246\n",
      "Epoch 36/40 - Train Loss: 0.1222\n",
      "Epoch 37/40 - Train Loss: 0.1197\n",
      "Epoch 38/40 - Train Loss: 0.1232\n",
      "Epoch 39/40 - Train Loss: 0.1163\n",
      "Epoch 40/40 - Train Loss: 0.1180\n",
      "Site: ../processed_ds/solar/solar_il/solar_il.csv, Test MAE: 0.3139\n",
      "\n",
      "Processing Site: ../processed_ds/solar/solar_me/solar_me.csv\n",
      "Train size: 1660 | Test size: 415\n",
      "Total trainable parameters: 3618\n",
      "Epoch 1/40 - Train Loss: 1.0352\n",
      "Epoch 2/40 - Train Loss: 0.9583\n",
      "Epoch 3/40 - Train Loss: 0.8526\n",
      "Epoch 4/40 - Train Loss: 0.6631\n",
      "Epoch 5/40 - Train Loss: 0.4883\n",
      "Epoch 6/40 - Train Loss: 0.3865\n",
      "Epoch 7/40 - Train Loss: 0.3318\n",
      "Epoch 8/40 - Train Loss: 0.3058\n",
      "Epoch 9/40 - Train Loss: 0.2910\n",
      "Epoch 10/40 - Train Loss: 0.2773\n",
      "Epoch 11/40 - Train Loss: 0.2623\n",
      "Epoch 12/40 - Train Loss: 0.2570\n",
      "Epoch 13/40 - Train Loss: 0.2433\n",
      "Epoch 14/40 - Train Loss: 0.2436\n",
      "Epoch 15/40 - Train Loss: 0.2307\n",
      "Epoch 16/40 - Train Loss: 0.2251\n",
      "Epoch 17/40 - Train Loss: 0.2235\n",
      "Epoch 18/40 - Train Loss: 0.2188\n",
      "Epoch 19/40 - Train Loss: 0.2119\n",
      "Epoch 20/40 - Train Loss: 0.2145\n",
      "Epoch 21/40 - Train Loss: 0.2031\n",
      "Epoch 22/40 - Train Loss: 0.2051\n",
      "Epoch 23/40 - Train Loss: 0.2009\n",
      "Epoch 24/40 - Train Loss: 0.2007\n",
      "Epoch 25/40 - Train Loss: 0.2013\n",
      "Epoch 26/40 - Train Loss: 0.1949\n",
      "Epoch 27/40 - Train Loss: 0.1925\n",
      "Epoch 28/40 - Train Loss: 0.1892\n",
      "Epoch 29/40 - Train Loss: 0.1920\n",
      "Epoch 30/40 - Train Loss: 0.1891\n",
      "Epoch 31/40 - Train Loss: 0.1908\n",
      "Epoch 32/40 - Train Loss: 0.1900\n",
      "Epoch 33/40 - Train Loss: 0.1873\n",
      "Epoch 34/40 - Train Loss: 0.1869\n",
      "Epoch 35/40 - Train Loss: 0.1838\n",
      "Epoch 36/40 - Train Loss: 0.1816\n",
      "Epoch 37/40 - Train Loss: 0.1821\n",
      "Epoch 38/40 - Train Loss: 0.1827\n",
      "Epoch 39/40 - Train Loss: 0.1797\n",
      "Epoch 40/40 - Train Loss: 0.1847\n",
      "Site: ../processed_ds/solar/solar_me/solar_me.csv, Test MAE: 0.4788\n",
      "\n",
      "Processing Site: ../processed_ds/solar/solar_ma/solar_ma.csv\n",
      "Train size: 1660 | Test size: 415\n",
      "Total trainable parameters: 3618\n",
      "Epoch 1/40 - Train Loss: 0.9407\n",
      "Epoch 2/40 - Train Loss: 0.8663\n",
      "Epoch 3/40 - Train Loss: 0.7205\n",
      "Epoch 4/40 - Train Loss: 0.5228\n",
      "Epoch 5/40 - Train Loss: 0.3917\n",
      "Epoch 6/40 - Train Loss: 0.3281\n",
      "Epoch 7/40 - Train Loss: 0.2939\n",
      "Epoch 8/40 - Train Loss: 0.2650\n",
      "Epoch 9/40 - Train Loss: 0.2456\n",
      "Epoch 10/40 - Train Loss: 0.2292\n",
      "Epoch 11/40 - Train Loss: 0.2160\n",
      "Epoch 12/40 - Train Loss: 0.2022\n",
      "Epoch 13/40 - Train Loss: 0.1912\n",
      "Epoch 14/40 - Train Loss: 0.1814\n",
      "Epoch 15/40 - Train Loss: 0.1742\n",
      "Epoch 16/40 - Train Loss: 0.1671\n",
      "Epoch 17/40 - Train Loss: 0.1649\n",
      "Epoch 18/40 - Train Loss: 0.1581\n",
      "Epoch 19/40 - Train Loss: 0.1562\n",
      "Epoch 20/40 - Train Loss: 0.1498\n",
      "Epoch 21/40 - Train Loss: 0.1496\n",
      "Epoch 22/40 - Train Loss: 0.1461\n",
      "Epoch 23/40 - Train Loss: 0.1407\n",
      "Epoch 24/40 - Train Loss: 0.1344\n",
      "Epoch 25/40 - Train Loss: 0.1367\n",
      "Epoch 26/40 - Train Loss: 0.1312\n",
      "Epoch 27/40 - Train Loss: 0.1353\n",
      "Epoch 28/40 - Train Loss: 0.1351\n",
      "Epoch 29/40 - Train Loss: 0.1329\n",
      "Epoch 30/40 - Train Loss: 0.1293\n",
      "Epoch 31/40 - Train Loss: 0.1274\n",
      "Epoch 32/40 - Train Loss: 0.1279\n",
      "Epoch 33/40 - Train Loss: 0.1287\n",
      "Epoch 34/40 - Train Loss: 0.1272\n",
      "Epoch 35/40 - Train Loss: 0.1290\n",
      "Epoch 36/40 - Train Loss: 0.1236\n",
      "Epoch 37/40 - Train Loss: 0.1245\n",
      "Epoch 38/40 - Train Loss: 0.1239\n",
      "Epoch 39/40 - Train Loss: 0.1222\n",
      "Epoch 40/40 - Train Loss: 0.1236\n",
      "Site: ../processed_ds/solar/solar_ma/solar_ma.csv, Test MAE: 0.6022\n",
      "\n",
      "Processing Site: ../processed_ds/solar/solar_ks/solar_ks.csv\n",
      "Train size: 1660 | Test size: 415\n",
      "Total trainable parameters: 3618\n",
      "Epoch 1/40 - Train Loss: 1.0170\n",
      "Epoch 2/40 - Train Loss: 0.9656\n",
      "Epoch 3/40 - Train Loss: 0.8699\n",
      "Epoch 4/40 - Train Loss: 0.6665\n",
      "Epoch 5/40 - Train Loss: 0.4730\n",
      "Epoch 6/40 - Train Loss: 0.3693\n",
      "Epoch 7/40 - Train Loss: 0.3161\n",
      "Epoch 8/40 - Train Loss: 0.2849\n",
      "Epoch 9/40 - Train Loss: 0.2702\n",
      "Epoch 10/40 - Train Loss: 0.2566\n",
      "Epoch 11/40 - Train Loss: 0.2444\n",
      "Epoch 12/40 - Train Loss: 0.2349\n",
      "Epoch 13/40 - Train Loss: 0.2244\n",
      "Epoch 14/40 - Train Loss: 0.2156\n",
      "Epoch 15/40 - Train Loss: 0.2072\n",
      "Epoch 16/40 - Train Loss: 0.1960\n",
      "Epoch 17/40 - Train Loss: 0.1944\n",
      "Epoch 18/40 - Train Loss: 0.1923\n",
      "Epoch 19/40 - Train Loss: 0.1844\n",
      "Epoch 20/40 - Train Loss: 0.1837\n",
      "Epoch 21/40 - Train Loss: 0.1747\n",
      "Epoch 22/40 - Train Loss: 0.1719\n",
      "Epoch 23/40 - Train Loss: 0.1650\n",
      "Epoch 24/40 - Train Loss: 0.1649\n",
      "Epoch 25/40 - Train Loss: 0.1603\n",
      "Epoch 26/40 - Train Loss: 0.1589\n",
      "Epoch 27/40 - Train Loss: 0.1561\n",
      "Epoch 28/40 - Train Loss: 0.1514\n",
      "Epoch 29/40 - Train Loss: 0.1517\n",
      "Epoch 30/40 - Train Loss: 0.1468\n",
      "Epoch 31/40 - Train Loss: 0.1471\n",
      "Epoch 32/40 - Train Loss: 0.1424\n",
      "Epoch 33/40 - Train Loss: 0.1402\n",
      "Epoch 34/40 - Train Loss: 0.1401\n",
      "Epoch 35/40 - Train Loss: 0.1368\n",
      "Epoch 36/40 - Train Loss: 0.1393\n",
      "Epoch 37/40 - Train Loss: 0.1348\n",
      "Epoch 38/40 - Train Loss: 0.1341\n",
      "Epoch 39/40 - Train Loss: 0.1338\n",
      "Epoch 40/40 - Train Loss: 0.1299\n",
      "Site: ../processed_ds/solar/solar_ks/solar_ks.csv, Test MAE: 1.1196\n",
      "\n",
      "Processing Site: ../processed_ds/solar/solar_fl/solar_fl.csv\n",
      "Train size: 1660 | Test size: 415\n",
      "Total trainable parameters: 3618\n",
      "Epoch 1/40 - Train Loss: 0.9957\n",
      "Epoch 2/40 - Train Loss: 0.9314\n",
      "Epoch 3/40 - Train Loss: 0.8156\n",
      "Epoch 4/40 - Train Loss: 0.5944\n",
      "Epoch 5/40 - Train Loss: 0.4408\n",
      "Epoch 6/40 - Train Loss: 0.3702\n",
      "Epoch 7/40 - Train Loss: 0.3156\n",
      "Epoch 8/40 - Train Loss: 0.2758\n",
      "Epoch 9/40 - Train Loss: 0.2462\n",
      "Epoch 10/40 - Train Loss: 0.2242\n",
      "Epoch 11/40 - Train Loss: 0.2098\n",
      "Epoch 12/40 - Train Loss: 0.1982\n",
      "Epoch 13/40 - Train Loss: 0.1921\n",
      "Epoch 14/40 - Train Loss: 0.1813\n",
      "Epoch 15/40 - Train Loss: 0.1780\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[136], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_experiment_mae_independent_lstm_detrend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[134], line 105\u001b[0m, in \u001b[0;36mrun_experiment_mae_independent_lstm_detrend\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, batch_y\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    104\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 105\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     epoch_train_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    107\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Documents/Arnob/centralized-baseline/centralized/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:130\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[1;32m    129\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Arnob/centralized-baseline/centralized/lib/python3.8/site-packages/torch/optim/optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Arnob/centralized-baseline/centralized/lib/python3.8/site-packages/torch/optim/optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/Documents/Arnob/centralized-baseline/centralized/lib/python3.8/site-packages/torch/optim/adam.py:200\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;129m@_use_grad_for_differentiable\u001b[39m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, closure\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    194\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform a single optimization step.\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m        closure (Callable, optional): A closure that reevaluates the model\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03m            and returns the loss.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_graph_capture_health_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Arnob/centralized-baseline/centralized/lib/python3.8/site-packages/torch/optim/optimizer.py:426\u001b[0m, in \u001b[0;36mOptimizer._cuda_graph_capture_health_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_cuda_graph_capture_health_check\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;66;03m# Note [torch.compile x capturable]\u001b[39;00m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;66;03m# If we are compiling, we try to take the capturable path automatically by\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;66;03m# Thus, when compiling, inductor will determine if cudagraphs\u001b[39;00m\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;66;03m# can be enabled based on whether there is input mutation or CPU tensors.\u001b[39;00m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m is_compiling()\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_built()\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available()\n\u001b[1;32m    425\u001b[0m     ):\n\u001b[0;32m--> 426\u001b[0m         capturing \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_current_stream_capturing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m capturing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    429\u001b[0m             group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups\n\u001b[1;32m    430\u001b[0m         ):\n\u001b[1;32m    431\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    432\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting CUDA graph capture of step() for an instance of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    433\u001b[0m                 \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    434\u001b[0m                 \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but param_groups\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m capturable is False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m             )\n",
      "File \u001b[0;32m~/Documents/Arnob/centralized-baseline/centralized/lib/python3.8/site-packages/torch/cuda/graphs.py:29\u001b[0m, in \u001b[0;36mis_current_stream_capturing\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mis_current_stream_capturing\u001b[39m():\n\u001b[1;32m     25\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return True if CUDA graph capture is underway on the current CUDA stream, False otherwise.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    If a CUDA context does not exist on the current device, returns False without initializing the context.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_cuda_isCurrentStreamCapturing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_experiment_mae_independent_lstm_detrend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "centralized",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
