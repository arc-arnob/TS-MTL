{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def load_existing_results(file_path=\"forecasting_results.json\"):\n",
    "    \"\"\"\n",
    "    Load existing results from a JSON file.\n",
    "    Returns an empty dictionary if the file doesn't exist.\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "\n",
    "def save_results_to_json(data, file_path=\"forecasting_results.json\"):\n",
    "    \"\"\"\n",
    "    Save the results dictionary to a JSON file, handling NumPy data types.\n",
    "    \"\"\"\n",
    "\n",
    "    # Handle NumPy data types (recursive conversion)\n",
    "    def convert_numpy(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            return {k: convert_numpy(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [convert_numpy(i) for i in obj]\n",
    "        elif isinstance(obj, (np.integer, np.int64, np.int32)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.floating, np.float64, np.float32)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()  # Convert arrays to lists\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "    # Convert data and save to JSON\n",
    "    data = convert_numpy(data)\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "    print(f\"âœ… Results saved to {file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "def store_results(dataset_name, horizons, horizon_value, experiment_type, backbone, mae_result, file_path=\"forecasting_results.json\"):\n",
    "    \"\"\"\n",
    "    Store MAE results for a given experiment type (stl_mae, mtl_mae, global_mae) per horizon.\n",
    "\n",
    "    Args:\n",
    "    - dataset_name (str): Name of the dataset (e.g., 'Solar', 'Air Quality').\n",
    "    - horizons (list): List of horizon values (e.g., [1, 2, 4, 8, 16]).\n",
    "    - horizon_value (int): The horizon corresponding to the mae_result provided.\n",
    "    - experiment_type (str): One of ['stl_mae', 'mtl_mae', 'global_mae'].\n",
    "    - backbone (str): Model backbone name (e.g., 'Deep_LSTM', 'simple_transformer').\n",
    "    - mae_result (list): MAE values for the current horizon (list of floats).\n",
    "    - file_path (str): JSON file to store the results.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Load existing results\n",
    "    results_dict = load_existing_results(file_path)\n",
    "\n",
    "    # Create dataset entry if it doesn't exist\n",
    "    dataset_key = f\"{dataset_name}_{backbone}\"\n",
    "    if dataset_key not in results_dict:\n",
    "        results_dict[dataset_key] = {\n",
    "            \"horizons\": horizons,\n",
    "            \"mtl\": [[] for _ in horizons],\n",
    "            \"global\": [[] for _ in horizons],\n",
    "            \"independent\": [[] for _ in horizons]\n",
    "        }\n",
    "\n",
    "    # Find index for the given horizon\n",
    "    try:\n",
    "        horizon_index = horizons.index(horizon_value)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"âš ï¸ Horizon value {horizon_value} not found in {horizons}.\")\n",
    "\n",
    "    # Append the mae_result to the correct horizon\n",
    "    results_dict[dataset_key][experiment_type][horizon_index].extend(mae_result)\n",
    "\n",
    "    # Save updated results\n",
    "    save_results_to_json(results_dict, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Requisite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import dateutil\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def df_to_X_y(df, features, target, window_size=32, horizon=1):\n",
    "    \"\"\"\n",
    "    Converts a DataFrame into supervised learning format for multi-step time series forecasting.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): DataFrame containing time series data.\n",
    "    - features (list): List of columns to use as features.\n",
    "    - target (str): The target column to predict.\n",
    "    - window_size (int): Past window size.\n",
    "    - horizon (int): Number of future steps.\n",
    "\n",
    "    Returns:\n",
    "    - X (np.array): Features (num_samples, window_size, num_features).\n",
    "    - y (np.array): Targets (num_samples, horizon).\n",
    "    \"\"\"\n",
    "    # Ensure target is in features\n",
    "    if target not in features:\n",
    "        features = [target] + features\n",
    "\n",
    "    data = df[features].to_numpy()  # Features including target's history\n",
    "    target_data = df[target].to_numpy()  # Target series\n",
    "\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size - horizon + 1):\n",
    "        # Past window_size feature values (including target)\n",
    "        X.append(data[i:i + window_size])\n",
    "\n",
    "        # Multi-step target: a sequence of future steps\n",
    "        y.append(target_data[i + window_size : i + window_size + horizon])\n",
    "\n",
    "    return np.array(X), np.array(y)  # y shape: (num_samples, horizon)\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Data Loader Function (Target included in features) ----------\n",
    "def load_and_preprocess_site_data(site_path, features, target, window_size=32, horizon=1, min_date=None, max_date=None, batch_size=16, device='cpu'):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses time series data for a given site with specified features and target,\n",
    "    ensuring the target column's historical values are included in the features.\n",
    "    \n",
    "    Args:\n",
    "    - site_path (str): Path to the CSV file.\n",
    "    - features (list): List of feature columns to use.\n",
    "    - target (str): Target column name.\n",
    "    - window_size (int): Past window size.\n",
    "    - horizon (int): Forecast horizon.\n",
    "    - min_date, max_date (str or datetime): Optional date filtering.\n",
    "    - batch_size (int): Batch size for DataLoader.\n",
    "    - device (str): 'cpu' or 'cuda'.\n",
    "\n",
    "    Returns:\n",
    "    - train_loader, val_loader, test_loader: PyTorch DataLoaders.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(site_path)\n",
    "\n",
    "    # Convert date column to datetime if exists\n",
    "    if 'date' in df.columns:\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        if min_date:\n",
    "            min_date = dateutil.parser.parse(min_date) if isinstance(min_date, str) else min_date\n",
    "            df = df[df['date'] >= min_date]\n",
    "        if max_date:\n",
    "            max_date = dateutil.parser.parse(max_date) if isinstance(max_date, str) else max_date\n",
    "            df = df[df['date'] <= max_date]\n",
    "        df.drop(columns=['date'], inplace=True)\n",
    "    # Ensure target is included in the feature set\n",
    "    if target not in features:\n",
    "        features = [target] + features\n",
    "\n",
    "    # Check for missing columns\n",
    "    all_columns = features\n",
    "    if not all(col in df.columns for col in all_columns):\n",
    "        missing = [col for col in all_columns if col not in df.columns]\n",
    "        raise ValueError(f\"Missing columns in dataset: {missing}\")\n",
    "\n",
    "    # Split data: 80% Train/Val, 20% Test\n",
    "    train_size = int(0.8 * len(df))\n",
    "    train_df = df.iloc[:train_size]\n",
    "    test_df = df.iloc[train_size:]\n",
    "\n",
    "    # 16% validation from train set\n",
    "    val_size = int(0.2 * len(train_df))\n",
    "    train_df, val_df = train_df.iloc[:-val_size], train_df.iloc[-val_size:]\n",
    "\n",
    "    print(f\"Train size: {len(train_df)} | Validation size: {len(val_df)} | Test size: {len(test_df)}\")\n",
    "\n",
    "    # # Standardization (using training stats)\n",
    "    train_mean, train_std = train_df[all_columns].mean(), train_df[all_columns].std()\n",
    "    train_df[all_columns] = (train_df[all_columns] - train_mean) / (train_std + 1e-8)\n",
    "    val_df[all_columns] = (val_df[all_columns] - train_mean) / (train_std + 1e-8)\n",
    "    test_df[all_columns] = (test_df[all_columns] - train_mean) / (train_std + 1e-8)\n",
    "    \n",
    "    # âœ… **Min-Max Scaling** (fitted on train only)\n",
    "    # scaler = MinMaxScaler()\n",
    "    # train_df[all_columns] = scaler.fit_transform(train_df[all_columns])\n",
    "    # val_df[all_columns] = scaler.transform(val_df[all_columns])\n",
    "    # test_df[all_columns] = scaler.transform(test_df[all_columns])\n",
    "\n",
    "    # Generate sequences\n",
    "    X_train, y_train = df_to_X_y(train_df, features, target, window_size, horizon)\n",
    "    X_val, y_val = df_to_X_y(val_df, features, target, window_size, horizon)\n",
    "    X_test, y_test = df_to_X_y(test_df, features, target, window_size, horizon)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    train_data = TensorDataset(torch.tensor(X_train, dtype=torch.float32).to(device), torch.tensor(y_train, dtype=torch.float32).to(device))\n",
    "    val_data = TensorDataset(torch.tensor(X_val, dtype=torch.float32).to(device), torch.tensor(y_val, dtype=torch.float32).to(device))\n",
    "    test_data = TensorDataset(torch.tensor(X_test, dtype=torch.float32).to(device), torch.tensor(y_test, dtype=torch.float32).to(device))\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "    val_loader = DataLoader(val_data, shuffle=False, batch_size=batch_size, drop_last=True)\n",
    "    test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader #,scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ SIMPLE MULTI-TASK TRANSFORMER MODEL ------------------\n",
    "\n",
    "class SimpleMultiTaskTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-task Transformer model with a shared encoder and task-specific decoders.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, d_model, nhead, num_layers, dim_feedforward, output_dim, num_tasks, dropout=0.1):\n",
    "        super(SimpleMultiTaskTransformer, self).__init__()\n",
    "        self.shared_encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.shared_encoder = nn.TransformerEncoder(self.shared_encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Linear projection to match d_model\n",
    "        self.input_projection = nn.Linear(input_dim, d_model)\n",
    "\n",
    "        # Task-specific decoders\n",
    "        self.task_heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(d_model, dim_feedforward),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(dim_feedforward, output_dim)\n",
    "            ) for _ in range(num_tasks)\n",
    "        ])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = []\n",
    "        for i, x in enumerate(inputs):\n",
    "            x = self.input_projection(x)\n",
    "            x = self.shared_encoder(x)\n",
    "            x = x[:, -1, :]  # Use the last time step output\n",
    "            outputs.append(self.task_heads[i](x))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better Transformer with Postional Encoding, Convolution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements positional encoding as used in Transformer architectures.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class EnhancedMultiTaskTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Enhanced Multi-task Transformer model with Conv1D embedding, positional encoding,\n",
    "    causal masking to prevent information leakage, shared Transformer encoder,\n",
    "    and task-specific decoders.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, d_model, nhead, num_layers, dim_feedforward, output_dim, num_tasks,\n",
    "                 seq_len=32, conv1d_emb=True, conv1d_kernel_size=3, dropout=0.1, device=\"cuda\"):\n",
    "        super(EnhancedMultiTaskTransformer, self).__init__()\n",
    "\n",
    "        self.conv1d_emb = conv1d_emb\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_size = d_model\n",
    "        self.device = device\n",
    "\n",
    "        # Input Embedding: Conv1D or Linear\n",
    "        if conv1d_emb:\n",
    "            if conv1d_kernel_size % 2 == 0:\n",
    "                raise Exception(\"conv1d_kernel_size must be an odd number to preserve dimensions.\")\n",
    "            self.conv1d_padding = conv1d_kernel_size - 1\n",
    "            self.input_embedding = nn.Conv1d(input_dim, d_model, kernel_size=conv1d_kernel_size)\n",
    "        else:\n",
    "            self.input_embedding = nn.Linear(input_dim, d_model)\n",
    "\n",
    "        # Positional Encoding\n",
    "        self.position_encoder = PositionalEncoding(d_model=d_model, dropout=dropout, max_len=seq_len)\n",
    "\n",
    "        # Shared Transformer Encoder\n",
    "        self.shared_encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.shared_encoder = nn.TransformerEncoder(self.shared_encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Task-specific decoders\n",
    "        self.task_heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(seq_len * d_model, dim_feedforward),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(dim_feedforward, output_dim)\n",
    "            ) for _ in range(num_tasks)\n",
    "        ])\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def _generate_square_subsequent_mask(self):\n",
    "        \"\"\"\n",
    "        Generates a causal mask to prevent attention to future positions.\n",
    "        \"\"\"\n",
    "        return torch.triu(\n",
    "            torch.full((self.seq_len, self.seq_len), float('-inf'), dtype=torch.float32, device=self.device),\n",
    "            diagonal=1,\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = []\n",
    "        src_mask = self._generate_square_subsequent_mask()\n",
    "        for i, x in enumerate(inputs):\n",
    "            if self.conv1d_emb:\n",
    "                x = F.pad(x, (0, 0, self.conv1d_padding, 0), \"constant\", -1)\n",
    "                x = self.input_embedding(x.transpose(1, 2)).transpose(1, 2)\n",
    "            else:\n",
    "                x = self.input_embedding(x)\n",
    "\n",
    "            x = self.position_encoder(x)\n",
    "            x = self.shared_encoder(x, mask=src_mask)\n",
    "            x = x.reshape(x.size(0), -1)  # Flatten all sequence outputs\n",
    "\n",
    "            outputs.append(self.task_heads[i](x))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ TRAINING & EVALUATION FOR SIMPLE MTL TRANSFORMER ------------------\n",
    "\n",
    "def train_simple_transformer(site_loaders, input_dim, d_model, nhead, num_layers, dim_feedforward, output_dim, num_tasks, num_epochs=5, device='cpu'):\n",
    "    \"\"\"\n",
    "    Trains the SimpleMultiTaskTransformer model across multiple site datasets.\n",
    "    \"\"\"\n",
    "    # model = SimpleMultiTaskTransformer(input_dim, d_model, nhead, num_layers, dim_feedforward, output_dim, num_tasks).to(device)\n",
    "    model = EnhancedMultiTaskTransformer(input_dim, d_model, nhead, num_layers, dim_feedforward, output_dim, num_tasks).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "    # Unpack loaders for each tasks\n",
    "    train_loaders = [loader_tuple[0] for loader_tuple in site_loaders]\n",
    "    val_loaders = [loader_tuple[1] for loader_tuple in site_loaders]\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        # Iterate over batches from all tasks simultaneously\n",
    "        for batches in zip(*train_loaders):\n",
    "            # Each batch in batches is a tuple (X, y) for a given task\n",
    "            Xs = [batch[0].to(device) for batch in batches]\n",
    "            ys = [batch[1].to(device) for batch in batches]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            # Pass the list of task batches to the model\n",
    "            preds_list = model(Xs)  # expects a list of tensors, one per task\n",
    "            \n",
    "            # Compute losses for each task and sum them\n",
    "            losses = [\n",
    "                criterion(pred, y.view(y.size(0), -1))\n",
    "                for pred, y in zip(preds_list, ys)\n",
    "            ]\n",
    "            total_loss = sum(losses)\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(total_loss.item())\n",
    "        \n",
    "        # Validation phase (similarly, iterate over all task validation loaders)\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batches in zip(*val_loaders):\n",
    "                Xs = [batch[0].to(device) for batch in batches]\n",
    "                ys = [batch[1].to(device) for batch in batches]\n",
    "                preds_list = model(Xs)\n",
    "                losses = [\n",
    "                    criterion(pred, y.view(y.size(0), -1)).item()\n",
    "                    for pred, y in zip(preds_list, ys)\n",
    "                ]\n",
    "                # Average loss over tasks for this batch\n",
    "                val_losses.append(sum(losses) / num_tasks)\n",
    "        \n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} | Train Loss: {np.mean(train_losses):.4f} | Validation Loss: {np.mean(val_losses):.4f}\")\n",
    "    \n",
    "    print(\"Training complete.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_simple_transformer(model, site_loaders, horizon_val, ds ='NK' ,device='cpu', horizon=16):\n",
    "    \"\"\"\n",
    "    Evaluates the SimpleMultiTaskTransformer model on the test set for all sites and computes MAE.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # Prepare test loaders from site_loaders\n",
    "    test_loaders = [loader_tuple[2] for loader_tuple in site_loaders]\n",
    "    task_preds, task_targets = [[] for _ in range(len(test_loaders))], [[] for _ in range(len(test_loaders))]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batches in zip(*test_loaders):\n",
    "            Xs = [batch[0].to(device) for batch in batches]\n",
    "            ys = [batch[1].to(device) for batch in batches]\n",
    "            preds_list = model(Xs)\n",
    "            for i, (pred, y) in enumerate(zip(preds_list, ys)):\n",
    "                task_preds[i].append(pred.cpu().numpy())\n",
    "                task_targets[i].append(y.cpu().numpy())\n",
    "    \n",
    "    # Compute MAE for each task\n",
    "    mae_scores = []\n",
    "    for preds, targets in zip(task_preds, task_targets):\n",
    "        preds_concat = np.concatenate(preds)\n",
    "        targets_concat = np.concatenate(targets)\n",
    "        mae_scores.append(mean_absolute_error(targets_concat, preds_concat))\n",
    "\n",
    "    print(\"Simple Transformer MTL evaluation complete.\")\n",
    "    # Append results to output.txt\n",
    "    with open(\"output_test.txt\", \"a\") as f:\n",
    "        f.write(f\"\\n==================== Simple TRANSFORMER MTL MODEL RESULTS {ds} ====================\\n\")\n",
    "        f.write(f\"MAE per task: {mae_scores}\\n\")\n",
    "    store_results(\n",
    "        dataset_name=ds,\n",
    "        horizons=[1,2,4,8,16],\n",
    "        experiment_type='mtl',\n",
    "        mae_result=mae_scores,\n",
    "        backbone='simple_transformer',\n",
    "        horizon_value=horizon_val\n",
    "    )\n",
    "    print(\"Results saved to output.txt.\")\n",
    "    return mae_scores\n",
    "\n",
    "\n",
    "def plot_simple_transformer_predictions(model, site_loaders, device='cpu'):\n",
    "    \"\"\"\n",
    "    Plots predictions vs. ground truth for each task in the SimpleMultiTaskTransformer model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    for task_id, (_, _, test_loader) in enumerate(site_loaders):\n",
    "        all_preds, all_truths = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in test_loader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                preds = model([X])[task_id]\n",
    "                all_preds.append(preds.cpu().numpy())\n",
    "                all_truths.append(y.cpu().numpy())\n",
    "\n",
    "        preds_concat = np.concatenate(all_preds, axis=0).flatten()\n",
    "        truths_concat = np.concatenate(all_truths, axis=0).flatten()\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(truths_concat, label=\"Ground Truth\", linewidth=1)\n",
    "        plt.plot(preds_concat, label=\"Transformer MTL Prediction\", linewidth=1, linestyle='--')\n",
    "        plt.title(f\"Transformer MTL - Task {task_id + 1}: Predictions vs Ground Truth\")\n",
    "        plt.xlabel(\"Sample Index\")\n",
    "        plt.ylabel(\"Value\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# ------------------ TRAINING PIPELINE FOR SIMPLE MTL TRANSFORMER ------------------\n",
    "\n",
    "def run_simple_transformer_pipeline(datasets, horizons, device='cpu'):\n",
    "    \"\"\"\n",
    "    Runs the full training, evaluation, and plotting pipeline for the SimpleMultiTaskTransformer model.\n",
    "    \"\"\"\n",
    "    for dataset in datasets:\n",
    "        print(f\"\\n==================== ğŸŒŸ DATASET: {dataset['ds']} ====================\")\n",
    "        for horizon in horizons:\n",
    "            print(f\"\\n==================== â³ HORIZON: {horizon} ====================\")\n",
    "\n",
    "            site_paths = [\n",
    "                os.path.join(root, file)\n",
    "                for root, dirs, files in os.walk(dataset['base_path'])\n",
    "                if root != dataset['base_path']\n",
    "                for file in files\n",
    "                if file.endswith(\".csv\")\n",
    "            ]\n",
    "\n",
    "            total_sites = len(site_paths)\n",
    "            num_tasks = total_sites\n",
    "            batch_size, window_size, input_dim, d_model, hidden_dim, output_dim = 32, 32, len(dataset['features']), 128, 512, horizon\n",
    "\n",
    "            site_loaders = [\n",
    "                load_and_preprocess_site_data(\n",
    "                    site_path,\n",
    "                    dataset['features'],\n",
    "                    dataset['target'],\n",
    "                    window_size,\n",
    "                    horizon=output_dim,\n",
    "                    batch_size=batch_size,\n",
    "                    min_date=dataset['min_date'],\n",
    "                    max_date=dataset['max_date']\n",
    "                ) for site_path in site_paths\n",
    "            ]\n",
    "\n",
    "            # Training Transformer MTL model\n",
    "            transformer_mtl_model = train_simple_transformer(site_loaders, input_dim, d_model, nhead=2, num_layers=1, dim_feedforward=hidden_dim, output_dim=output_dim, num_tasks=num_tasks, num_epochs=10, device=device)\n",
    "            # Evaluating Transformer MTL model\n",
    "            transformer_mtl_mae = evaluate_simple_transformer(transformer_mtl_model, site_loaders, horizon_val=horizon ,device=device, ds = dataset['ds'])\n",
    "            # Plotting predictions\n",
    "            # plot_simple_transformer_predictions(transformer_mtl_model, site_loaders, device=device)\n",
    "\n",
    "            print(f\"âœ… Completed: {dataset['ds']} | Horizon: {horizon} | Transformer MTL MAE per task: {transformer_mtl_mae}\")\n",
    "\n",
    "    print(\"\\nğŸ† All Transformer MTL experiments completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizons = [1, 2, 4, 8, 16]\n",
    "\n",
    "# ğŸŒ Dataset Configurations\n",
    "datasets = [\n",
    "    # {\n",
    "    #     'ds': 'Solar',\n",
    "    #     'features': ['loc-1', 'loc-2', 'loc-3', 'loc-4'],\n",
    "    #     'target': 'loc-1',\n",
    "    #     'base_path': \"../processed_ds/solar/\",\n",
    "    #     'min_date': \"2006-09-01\",\n",
    "    #     'max_date': \"2006-09-08 4:50\"\n",
    "    # },\n",
    "    {\n",
    "        'ds': 'Air Quality',\n",
    "        'features': ['PM2.5', 'OT', 'PM10', 'NO2'],\n",
    "        'target': 'PM2.5',\n",
    "        'base_path': '../processed_ds/air_quality_cluster',\n",
    "        'min_date': \"2014-09-01\",\n",
    "        'max_date': \"2014-11-12 19:00\"\n",
    "    },\n",
    "    # {\n",
    "    #     'ds': 'Crypto',\n",
    "    #     'features': ['Open', 'High', 'Low', 'OT', 'Volume'],\n",
    "    #     'target': 'OT',\n",
    "    #     'base_path': \"../processed_ds/crypto-data/\",\n",
    "    #     'min_date': \"2018-04-01\",\n",
    "    #     'max_date': \"2018-06-15\"\n",
    "    # },\n",
    "    # {\n",
    "    #     'ds': 'Sales',\n",
    "    #     'features': ['OT', 'customers', 'open', 'promo', 'holiday'],\n",
    "    #     'target': 'OT',\n",
    "    #     'base_path': \"../processed_ds/stores_data/\",\n",
    "    #     'min_date': \"2013-01-16\",\n",
    "    #     'max_date': \"2015-07-31\"\n",
    "    # }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== ğŸŒŸ DATASET: Air Quality ====================\n",
      "\n",
      "==================== â³ HORIZON: 1 ====================\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Epoch 1/10 | Train Loss: 33.0085 | Validation Loss: 0.7688\n",
      "Epoch 2/10 | Train Loss: 2.1055 | Validation Loss: 0.5624\n",
      "Epoch 3/10 | Train Loss: 1.4313 | Validation Loss: 0.4557\n",
      "Epoch 4/10 | Train Loss: 1.1152 | Validation Loss: 0.3609\n",
      "Epoch 5/10 | Train Loss: 0.9437 | Validation Loss: 0.2989\n",
      "Epoch 6/10 | Train Loss: 0.8761 | Validation Loss: 0.2802\n",
      "Epoch 7/10 | Train Loss: 0.7652 | Validation Loss: 0.2661\n",
      "Epoch 8/10 | Train Loss: 0.7141 | Validation Loss: 0.2656\n",
      "Epoch 9/10 | Train Loss: 0.6631 | Validation Loss: 0.2443\n",
      "Epoch 10/10 | Train Loss: 0.6548 | Validation Loss: 0.2809\n",
      "Training complete.\n",
      "Simple Transformer MTL evaluation complete.\n",
      "âœ… Results saved to forecasting_results.json\n",
      "Results saved to output.txt.\n",
      "âœ… Completed: Air Quality | Horizon: 1 | Transformer MTL MAE per task: [0.16171676, 0.26070258, 0.14909728, 0.16444199, 0.121248655, 0.15462653, 0.2607098, 0.18374096, 0.26724795, 0.26971963, 0.17010485, 0.17536837]\n",
      "\n",
      "==================== â³ HORIZON: 2 ====================\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Epoch 1/10 | Train Loss: 22.3107 | Validation Loss: 0.7248\n",
      "Epoch 2/10 | Train Loss: 1.8812 | Validation Loss: 0.6239\n",
      "Epoch 3/10 | Train Loss: 1.3924 | Validation Loss: 0.4947\n",
      "Epoch 4/10 | Train Loss: 1.1280 | Validation Loss: 0.4712\n",
      "Epoch 5/10 | Train Loss: 0.9728 | Validation Loss: 0.4605\n",
      "Epoch 6/10 | Train Loss: 0.8586 | Validation Loss: 0.4579\n",
      "Epoch 7/10 | Train Loss: 0.7934 | Validation Loss: 0.4006\n",
      "Epoch 8/10 | Train Loss: 0.7742 | Validation Loss: 0.4066\n",
      "Epoch 9/10 | Train Loss: 0.7166 | Validation Loss: 0.4211\n",
      "Epoch 10/10 | Train Loss: 0.6808 | Validation Loss: 0.3905\n",
      "Training complete.\n",
      "Simple Transformer MTL evaluation complete.\n",
      "âœ… Results saved to forecasting_results.json\n",
      "Results saved to output.txt.\n",
      "âœ… Completed: Air Quality | Horizon: 2 | Transformer MTL MAE per task: [0.17528808, 0.32900083, 0.21090955, 0.27994442, 0.1673468, 0.20958222, 0.30149114, 0.19271147, 0.29973397, 0.3688226, 0.26544, 0.2406739]\n",
      "\n",
      "==================== â³ HORIZON: 4 ====================\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Epoch 1/10 | Train Loss: 11.7140 | Validation Loss: 0.9107\n",
      "Epoch 2/10 | Train Loss: 2.1451 | Validation Loss: 0.7293\n",
      "Epoch 3/10 | Train Loss: 1.5732 | Validation Loss: 0.6039\n",
      "Epoch 4/10 | Train Loss: 1.2893 | Validation Loss: 0.6653\n",
      "Epoch 5/10 | Train Loss: 1.2254 | Validation Loss: 0.6553\n",
      "Epoch 6/10 | Train Loss: 1.0866 | Validation Loss: 0.6470\n",
      "Epoch 7/10 | Train Loss: 1.0243 | Validation Loss: 0.6366\n",
      "Epoch 8/10 | Train Loss: 0.9237 | Validation Loss: 0.6291\n",
      "Epoch 9/10 | Train Loss: 0.8651 | Validation Loss: 0.6555\n",
      "Epoch 10/10 | Train Loss: 0.8287 | Validation Loss: 0.6332\n",
      "Training complete.\n",
      "Simple Transformer MTL evaluation complete.\n",
      "âœ… Results saved to forecasting_results.json\n",
      "Results saved to output.txt.\n",
      "âœ… Completed: Air Quality | Horizon: 4 | Transformer MTL MAE per task: [0.23738125, 0.30988914, 0.30943194, 0.31901076, 0.22599056, 0.24885595, 0.3495447, 0.24787584, 0.3551572, 0.39597848, 0.28726384, 0.26702696]\n",
      "\n",
      "==================== â³ HORIZON: 8 ====================\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Epoch 1/10 | Train Loss: 7.8664 | Validation Loss: 1.0812\n",
      "Epoch 2/10 | Train Loss: 2.5274 | Validation Loss: 1.0275\n",
      "Epoch 3/10 | Train Loss: 2.0440 | Validation Loss: 1.0088\n",
      "Epoch 4/10 | Train Loss: 1.7481 | Validation Loss: 1.0030\n",
      "Epoch 5/10 | Train Loss: 1.5936 | Validation Loss: 1.0386\n",
      "Epoch 6/10 | Train Loss: 1.4739 | Validation Loss: 1.0231\n",
      "Epoch 7/10 | Train Loss: 1.3088 | Validation Loss: 1.0308\n",
      "Epoch 8/10 | Train Loss: 1.1614 | Validation Loss: 1.1116\n",
      "Epoch 9/10 | Train Loss: 1.0948 | Validation Loss: 1.0539\n",
      "Epoch 10/10 | Train Loss: 1.0192 | Validation Loss: 1.0353\n",
      "Training complete.\n",
      "Simple Transformer MTL evaluation complete.\n",
      "âœ… Results saved to forecasting_results.json\n",
      "Results saved to output.txt.\n",
      "âœ… Completed: Air Quality | Horizon: 8 | Transformer MTL MAE per task: [0.31874937, 0.4039468, 0.36651233, 0.38755485, 0.2932632, 0.33029938, 0.45171157, 0.3581463, 0.5098723, 0.6121563, 0.36847547, 0.30408853]\n",
      "\n",
      "==================== â³ HORIZON: 16 ====================\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Train size: 1119 | Validation size: 279 | Test size: 350\n",
      "Epoch 1/10 | Train Loss: 6.7968 | Validation Loss: 1.7904\n",
      "Epoch 2/10 | Train Loss: 3.2954 | Validation Loss: 1.5698\n",
      "Epoch 3/10 | Train Loss: 2.6450 | Validation Loss: 1.6061\n",
      "Epoch 4/10 | Train Loss: 2.3388 | Validation Loss: 1.7967\n",
      "Epoch 5/10 | Train Loss: 2.0176 | Validation Loss: 1.7145\n",
      "Epoch 6/10 | Train Loss: 1.8182 | Validation Loss: 1.6111\n",
      "Epoch 7/10 | Train Loss: 1.6237 | Validation Loss: 1.6770\n",
      "Epoch 8/10 | Train Loss: 1.4562 | Validation Loss: 1.7114\n",
      "Epoch 9/10 | Train Loss: 1.2738 | Validation Loss: 1.5959\n",
      "Epoch 10/10 | Train Loss: 1.1754 | Validation Loss: 1.6585\n",
      "Training complete.\n",
      "Simple Transformer MTL evaluation complete.\n",
      "âœ… Results saved to forecasting_results.json\n",
      "Results saved to output.txt.\n",
      "âœ… Completed: Air Quality | Horizon: 16 | Transformer MTL MAE per task: [0.39973596, 0.54873884, 0.4505173, 0.3949841, 0.4043762, 0.43954408, 0.51524675, 0.40907335, 0.7136495, 0.53554666, 0.47406486, 0.41970825]\n",
      "\n",
      "ğŸ† All Transformer MTL experiments completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "run_simple_transformer_pipeline(datasets=datasets, horizons=horizons, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "centralized",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
