{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def load_existing_results(file_path=\"forecasting_results.json\"):\n",
    "    \"\"\"\n",
    "    Load existing results from a JSON file.\n",
    "    Returns an empty dictionary if the file doesn't exist.\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "\n",
    "def save_results_to_json(data, file_path=\"forecasting_results.json\"):\n",
    "    \"\"\"\n",
    "    Save the results dictionary to a JSON file, handling NumPy data types.\n",
    "    \"\"\"\n",
    "\n",
    "    # Handle NumPy data types (recursive conversion)\n",
    "    def convert_numpy(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            return {k: convert_numpy(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [convert_numpy(i) for i in obj]\n",
    "        elif isinstance(obj, (np.integer, np.int64, np.int32)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.floating, np.float64, np.float32)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()  # Convert arrays to lists\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "    # Convert data and save to JSON\n",
    "    data = convert_numpy(data)\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "    print(f\"✅ Results saved to {file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "def store_results(dataset_name, horizons, horizon_value, experiment_type, backbone, mae_result, file_path=\"forecasting_results.json\"):\n",
    "    \"\"\"\n",
    "    Store MAE results for a given experiment type (stl_mae, mtl_mae, global_mae) per horizon.\n",
    "\n",
    "    Args:\n",
    "    - dataset_name (str): Name of the dataset (e.g., 'Solar', 'Air Quality').\n",
    "    - horizons (list): List of horizon values (e.g., [1, 2, 4, 8, 16]).\n",
    "    - horizon_value (int): The horizon corresponding to the mae_result provided.\n",
    "    - experiment_type (str): One of ['stl_mae', 'mtl_mae', 'global_mae'].\n",
    "    - backbone (str): Model backbone name (e.g., 'Deep_LSTM', 'simple_transformer').\n",
    "    - mae_result (list): MAE values for the current horizon (list of floats).\n",
    "    - file_path (str): JSON file to store the results.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Load existing results\n",
    "    results_dict = load_existing_results(file_path)\n",
    "\n",
    "    # Create dataset entry if it doesn't exist\n",
    "    dataset_key = f\"{dataset_name}_{backbone}\"\n",
    "    if dataset_key not in results_dict:\n",
    "        results_dict[dataset_key] = {\n",
    "            \"horizons\": horizons,\n",
    "            \"mtl\": [[] for _ in horizons],\n",
    "            \"global\": [[] for _ in horizons],\n",
    "            \"independent\": [[] for _ in horizons]\n",
    "        }\n",
    "\n",
    "    # Find index for the given horizon\n",
    "    try:\n",
    "        horizon_index = horizons.index(horizon_value)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"⚠️ Horizon value {horizon_value} not found in {horizons}.\")\n",
    "\n",
    "    # Append the mae_result to the correct horizon\n",
    "    results_dict[dataset_key][experiment_type][horizon_index].extend(mae_result)\n",
    "\n",
    "    # Save updated results\n",
    "    save_results_to_json(results_dict, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import dateutil.parser\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def df_to_X_y(df, features, target, window_size=32, horizon=1):\n",
    "    if target not in features:\n",
    "        features = [target] + features\n",
    "\n",
    "    data = df[features].to_numpy()\n",
    "    target_data = df[target].to_numpy()\n",
    "\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size - horizon + 1):\n",
    "        X.append(data[i:i + window_size])\n",
    "        y.append(target_data[i + window_size: i + window_size + horizon])\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- #\n",
    "#          LSTM Model         #\n",
    "# --------------------------- #\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, num_features, time_window, output_window, num_labels, num_layers=2, hidden_size=16, dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.output_window = output_window\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        self.lstm = nn.LSTM(num_features,\n",
    "                            hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True,\n",
    "                            dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, num_labels * output_window)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, D = x.shape  \n",
    "        x_, _ = self.lstm(x)\n",
    "        last_hidden = x_[:, -1, :]  # Take last time step's hidden state\n",
    "        x_ = self.fc(last_hidden)\n",
    "        x_ = x_.reshape(B, self.output_window, self.num_labels)\n",
    "        return x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------- #\n",
    "#        Data Preprocessing   #\n",
    "# --------------------------- #\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import dateutil.parser\n",
    "class BalancedDataLoaderIterator:\n",
    "    '''\n",
    "    This class iterates over multiple PyTorch dataloaders in a balanced way, \n",
    "    ensuring that all dataloaders contribute to the training equally. \n",
    "    It uses random sampling to choose from multiple dataloaders while \n",
    "    handling different dataset lengths.\n",
    "    '''\n",
    "    def __init__(self, dataloaders):\n",
    "        self.dataloaders = dataloaders\n",
    "\n",
    "        self.num_dataloaders = len(dataloaders)\n",
    "\n",
    "        max_length = max(len(dataloader) for dataloader in dataloaders)\n",
    "\n",
    "        length_list = [len(dataloader) for dataloader in dataloaders]\n",
    "        print(\"data loader length:\", length_list)\n",
    "        print(\"max dataloader length:\", max_length,\n",
    "              \"epoch iteration:\", max_length * self.num_dataloaders)\n",
    "        self.total_length = max_length * self.num_dataloaders\n",
    "        self.current_iteration = 0\n",
    "        self.probabilities = torch.ones(\n",
    "            self.num_dataloaders, dtype=torch.float) / self.num_dataloaders\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.iterators = [iter(dataloader) for dataloader in self.dataloaders]\n",
    "        self.current_iteration = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.current_iteration >= self.total_length:\n",
    "            raise StopIteration\n",
    "\n",
    "        chosen_index = torch.multinomial(self.probabilities, 1).item()\n",
    "        try:\n",
    "            sample = next(self.iterators[chosen_index])\n",
    "        except StopIteration:\n",
    "            self.iterators[chosen_index] = iter(self.dataloaders[chosen_index])\n",
    "            sample = next(self.iterators[chosen_index])\n",
    "\n",
    "        self.current_iteration += 1\n",
    "        return sample, chosen_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_length\n",
    "    \n",
    "    \n",
    "def load_all_sites_balanced(\n",
    "    base_dir='../processed_ds/air_quality_cluster/', \n",
    "    features= [],\n",
    "    target = '',\n",
    "    window_size=32, \n",
    "    horizon=16, \n",
    "    batch_size=16,\n",
    "    min_date=None, \n",
    "    max_date=None,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses time series data for all sites, ensuring balanced training across sites.\n",
    "\n",
    "    Args:\n",
    "    - base_dir (str): Path to the directory containing site folders.\n",
    "    - window_size (int): Number of past time steps for input.\n",
    "    - horizon (int): Number of future steps to predict.\n",
    "    - batch_size (int): Batch size for DataLoader.\n",
    "    - min_date (str or datetime, optional): Start date for filtering data.\n",
    "    - max_date (str or datetime, optional): End date for filtering data.\n",
    "\n",
    "    Returns:\n",
    "    - balanced_train_loader: Balanced DataLoader across all sites for training.\n",
    "    - site_val_loaders: Dictionary of validation DataLoaders per site.\n",
    "    - site_test_loaders: Dictionary of test DataLoaders per site.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the list of site directories\n",
    "    site_dirs = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "    site_dirs.sort()\n",
    "\n",
    "    train_loaders = []\n",
    "    site_val_loaders = {}\n",
    "    site_test_loaders = {}\n",
    "\n",
    "    for site in site_dirs:\n",
    "        site_path = os.path.join(base_dir, site)\n",
    "        csv_files = [f for f in os.listdir(site_path) if f.endswith('.csv')]\n",
    "        csv_files.sort()\n",
    "\n",
    "        for csv_file in csv_files:\n",
    "            file_path = os.path.join(site_path, csv_file)\n",
    "            df_site = pd.read_csv(file_path)\n",
    "\n",
    "            # Convert date column to datetime if it exists\n",
    "            if 'date' in df_site.columns:\n",
    "                df_site['date'] = pd.to_datetime(df_site['date'])\n",
    "\n",
    "                # Filter based on min_date and max_date\n",
    "                if min_date:\n",
    "                    min_date = dateutil.parser.parse(min_date) if isinstance(min_date, str) else min_date\n",
    "                    df_site = df_site[df_site['date'] >= min_date]\n",
    "\n",
    "                if max_date:\n",
    "                    max_date = dateutil.parser.parse(max_date) if isinstance(max_date, str) else max_date\n",
    "                    df_site = df_site[df_site['date'] <= max_date]\n",
    "\n",
    "                # Drop date column after filtering\n",
    "                df_site.drop(columns=['date'], inplace=True)\n",
    "\n",
    "            # Perform 80-20 train-test split\n",
    "            train_size = int(0.8 * len(df_site))\n",
    "            train_df = df_site.iloc[:train_size]\n",
    "            test_df = df_site.iloc[train_size:]\n",
    "\n",
    "            # Split train_df further into Train (80%) and Validation (20%)\n",
    "            val_size = int(0.2 * len(train_df))  # 16% of full dataset\n",
    "            train_df, val_df = train_df.iloc[:-val_size], train_df.iloc[-val_size:]\n",
    "\n",
    "            print(f\"Site: {site} | Train: {len(train_df)} | Validation: {len(val_df)} | Test: {len(test_df)}\")\n",
    "\n",
    "            # Standardize data per site (avoid data leakage)\n",
    "            train_mean, train_std = train_df.mean(), train_df.std()\n",
    "            train_df = (train_df - train_mean) / (train_std + 1e-8)\n",
    "            val_df = (val_df - train_mean) / (train_std + 1e-8)\n",
    "            test_df = (test_df - train_mean) / (train_std + 1e-8)\n",
    "\n",
    "            # Convert DataFrame to NumPy arrays for LSTM\n",
    "            X_train, y_train = df_to_X_y(train_df, features, target, window_size, horizon)\n",
    "            X_val, y_val = df_to_X_y(val_df,features, target ,window_size, horizon)\n",
    "            X_test, y_test = df_to_X_y(test_df, features, target ,window_size, horizon)\n",
    "\n",
    "            # Convert to PyTorch tensors\n",
    "            # train_data = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float())\n",
    "            # val_data = TensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).float())\n",
    "            # test_data = TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).float())\n",
    "            \n",
    "            train_data = TensorDataset(torch.tensor(X_train, dtype=torch.float32).to(device), torch.tensor(y_train, dtype=torch.float32).to(device))\n",
    "            val_data = TensorDataset(torch.tensor(X_val, dtype=torch.float32).to(device), torch.tensor(y_val, dtype=torch.float32).to(device))\n",
    "            test_data = TensorDataset(torch.tensor(X_test, dtype=torch.float32).to(device), torch.tensor(y_test, dtype=torch.float32).to(device))\n",
    "\n",
    "            # Create DataLoaders\n",
    "            train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "            val_loader = DataLoader(val_data, shuffle=False, batch_size=batch_size, drop_last=True)\n",
    "            test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "            # Store loaders for this site\n",
    "            train_loaders.append(train_loader)\n",
    "            site_val_loaders[site] = val_loader\n",
    "            site_test_loaders[site] = test_loader\n",
    "\n",
    "    # Create Balanced DataLoader for Training\n",
    "    balanced_train_loader = BalancedDataLoaderIterator(train_loaders)\n",
    "\n",
    "    return balanced_train_loader, site_val_loaders, site_test_loaders\n",
    "\n",
    "def train_model_balanced(model, balanced_train_loader, site_val_loaders, num_epochs=30, lr=1e-3, wd=1e-5):\n",
    "    \"\"\"\n",
    "    Trains an LSTM model using a balanced data loader (across multiple sites) and evaluates on\n",
    "    separate site-specific validation loaders to prevent data leakage.\n",
    "\n",
    "    Args:\n",
    "      model (nn.Module): The LSTM model.\n",
    "      balanced_train_loader (BalancedDataLoaderIterator): Iterator that yields (batch, chosen_index)\n",
    "            from a list of site-specific training DataLoaders.\n",
    "      site_val_loaders (dict): A dictionary mapping site identifiers to validation DataLoaders.\n",
    "      num_epochs (int): Number of training epochs.\n",
    "      lr (float): Learning rate.\n",
    "      wd (float): Weight decay.\n",
    "      \n",
    "    Returns:\n",
    "      train_losses (list): Average training loss per epoch.\n",
    "      val_losses (list): Average validation loss per epoch (aggregated across sites).\n",
    "    \"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    device = \"cuda\"\n",
    "    model.to(device)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        \n",
    "        # Iterate over the balanced training data\n",
    "        for (batch, chosen_index) in balanced_train_loader:\n",
    "            batch_x, batch_y = batch\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_x)\n",
    "            # Ensure that output and target shapes match: unsqueeze target if necessary\n",
    "            loss = criterion(output, batch_y.unsqueeze(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = running_train_loss / len(balanced_train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # -------------------- Validation (no data leak) -------------------- #\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Loop over each site's validation loader separately\n",
    "            for site, val_loader in site_val_loaders.items():\n",
    "                for batch_x, batch_y in val_loader:\n",
    "                    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                    output = model(batch_x)\n",
    "                    loss = criterion(output, batch_y.unsqueeze(-1))\n",
    "                    running_val_loss += loss.item()\n",
    "                    val_batches += 1\n",
    "        \n",
    "        avg_val_loss = running_val_loss / val_batches if val_batches > 0 else float('inf')\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "         # -------------------- Plot Loss Curves -------------------- #\n",
    "\n",
    "    # plt.figure(figsize=(8, 5))\n",
    "    # plt.plot(range(1, num_epochs + 1), train_losses, label=\"Training Loss\", marker='o')\n",
    "    # plt.plot(range(1, num_epochs + 1), val_losses, label=\"Validation Loss\", marker='s')\n",
    "    \n",
    "    # plt.xlabel(\"Epoch\")\n",
    "    # plt.ylabel(\"Loss\")\n",
    "    # plt.title(\"Training and Validation Loss Curves\")\n",
    "    # plt.legend()\n",
    "    # plt.grid(True)\n",
    "    # plt.show()\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "def evaluate_model_balanced(model, site_test_loaders):\n",
    "    \"\"\"\n",
    "    Evaluates the LSTM model on test data from each site separately to avoid data leakage.\n",
    "\n",
    "    Args:\n",
    "      model (nn.Module): Trained LSTM model.\n",
    "      site_test_loaders (dict): Dictionary mapping site identifiers to test DataLoaders.\n",
    "\n",
    "    Returns:\n",
    "      site_mae_list (list): List containing MAE for each site.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    criterion = nn.L1Loss(reduction='mean')\n",
    "    site_mae_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for site, test_loader in site_test_loaders.items():\n",
    "            site_preds, site_targets = [], []\n",
    "            for batch_x, batch_y in test_loader:\n",
    "                batch_x, batch_y = batch_x.to(\"cuda\"), batch_y.to(\"cuda\")\n",
    "                predictions = model(batch_x)\n",
    "                site_preds.append(predictions.cpu())\n",
    "                site_targets.append(batch_y.unsqueeze(-1).cpu())\n",
    "            \n",
    "            # Concatenate predictions and targets for full-site evaluation\n",
    "            site_preds = torch.cat(site_preds, dim=0)\n",
    "            site_targets = torch.cat(site_targets, dim=0)\n",
    "            site_mae = criterion(site_preds, site_targets).item()\n",
    "\n",
    "            print(f\"Site: {site}, Test MAE: {site_mae:.4f}\")\n",
    "            site_mae_list.append(site_mae)\n",
    "    \n",
    "    return site_mae_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def run_global_model_experiment_lstm():\n",
    "    \"\"\"\n",
    "    Runs a global LSTM model experiment by concatenating all site data for multiple datasets and horizons.\n",
    "    Appends site-wise and mean MAE results to output.txt.\n",
    "    \"\"\"\n",
    "    datasets = [\n",
    "        {\n",
    "            'name': 'Air Quality',\n",
    "            'features': ['PM2.5', 'OT', 'PM10', 'NO2'],\n",
    "            'target': 'PM2.5',\n",
    "            'directory': \"../processed_ds/air_quality_cluster/\",\n",
    "            'min_date': \"2014-09-01\",\n",
    "            'max_date': \"2014-11-12 19:00\",\n",
    "            'num_features': 4\n",
    "        },\n",
    "        {\n",
    "            'name': 'Solar',\n",
    "            'directory': \"../processed_ds/solar/\",\n",
    "            'features': ['loc-1', 'loc-2', 'loc-3', 'loc-4'],\n",
    "            'target': 'loc-1',\n",
    "            'min_date': \"2006-09-01\",\n",
    "            'max_date': \"2006-09-08 4:50\",\n",
    "            'num_features': 4\n",
    "        },\n",
    "        {\n",
    "            'name': 'Crypto',\n",
    "            'directory': \"../processed_ds/crypto-data/\",\n",
    "            'features': ['Open', 'High', 'Low', 'OT', 'Volume'],\n",
    "            'target': 'OT',\n",
    "            'min_date': \"2018-04-01\",\n",
    "            'max_date': \"2018-06-15\",\n",
    "            'num_features': 5\n",
    "        },\n",
    "        # {\n",
    "        #     'name': 'Sales',\n",
    "        #     'directory': \"../processed_ds/stores_data/\",\n",
    "        #     'min_date': \"2013-01-16\",\n",
    "        #     'max_date': \"2015-07-31\",\n",
    "        #     'num_features': 7\n",
    "        # }\n",
    "    ]\n",
    "\n",
    "    horizons = [1, 2, 4, 8, 16]\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    seq_len = 32\n",
    "    batch_size = 32\n",
    "    num_epochs = 30\n",
    "\n",
    "    for dataset in datasets:\n",
    "        print(f\"\\n==================== Dataset: {dataset['name']} ====================\")\n",
    "\n",
    "        for horizon in horizons:\n",
    "            print(f\"\\n--- Running Global Model for Horizon: {horizon} ---\")\n",
    "\n",
    "            # Load balanced data (all sites concatenated)\n",
    "            balanced_train_loader, val_loaders, test_loaders = load_all_sites_balanced(\n",
    "                base_dir=dataset['directory'],\n",
    "                features=dataset['features'],\n",
    "                target=dataset['target'],\n",
    "                window_size=seq_len,\n",
    "                horizon=horizon,\n",
    "                batch_size=batch_size,\n",
    "                min_date=dataset['min_date'],\n",
    "                max_date=dataset['max_date'],\n",
    "                device=device\n",
    "            )\n",
    "\n",
    "            # Global LSTM model definition\n",
    "            model = LSTMModel(\n",
    "                num_features=dataset['num_features'],\n",
    "                time_window=seq_len,\n",
    "                output_window=horizon,\n",
    "                num_labels=1,\n",
    "                num_layers=2,\n",
    "                hidden_size=8,\n",
    "                dropout=0.5\n",
    "            )\n",
    "            total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "            print(\"Total trainable parameters:\", total_params)\n",
    "            model.to(device)\n",
    "\n",
    "            # Train the global model\n",
    "            train_model_balanced(\n",
    "                model, \n",
    "                balanced_train_loader, \n",
    "                val_loaders, \n",
    "                num_epochs=num_epochs, \n",
    "                lr=1e-4, \n",
    "                wd=1e-5\n",
    "            )\n",
    "\n",
    "            # Evaluate the model on test data\n",
    "            mae_per_site = evaluate_model_balanced(model, test_loaders)\n",
    "            avg_mae = np.mean(mae_per_site)\n",
    "\n",
    "            # Output results\n",
    "            print(f\"Site-wise MAE for {dataset['name']} at horizon {horizon}: {mae_per_site}\")\n",
    "            print(f\"Average MAE for {dataset['name']} at horizon {horizon}: {avg_mae:.4f}\")\n",
    "\n",
    "            # Append results to output.txt\n",
    "            with open(\"output_test.txt\", \"a\") as f:\n",
    "                f.write(\"\\n==================== Simple GLOBAL LSTM MODEL RESULTS ====================\\n\")\n",
    "                f.write(f\"Dataset: {dataset['name']}\\n\")\n",
    "                f.write(f\"Horizon: {horizon}\\n\")\n",
    "                f.write(f\"MAE per site: {mae_per_site}\\n\")\n",
    "                f.write(f\"Mean MAE: {avg_mae:.4f}\\n\")\n",
    "            store_results(\n",
    "                dataset_name=dataset['name'],\n",
    "                horizons=[1,2,4,8,16],\n",
    "                experiment_type='global',\n",
    "                mae_result=mae_per_site,\n",
    "                backbone='simple_lstm',\n",
    "                horizon_value=horizon\n",
    "            )\n",
    "\n",
    "    print(\"\\n🏆 All Global LSTM Forecasting Model experiments are complete! 🏆\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Dataset: Air Quality ====================\n",
      "\n",
      "--- Running Global Model for Horizon: 1 ---\n",
      "Site: site-1 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-10 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-11 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-12 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-2 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-3 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-4 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-5 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-6 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-7 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-8 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-9 | Train: 1119 | Validation: 279 | Test: 350\n",
      "data loader length: [33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33]\n",
      "max dataloader length: 33 epoch iteration: 396\n",
      "Total trainable parameters: 1033\n",
      "Epoch 1 | Train Loss: 1.0049 | Val Loss: 2.5852\n",
      "Epoch 2 | Train Loss: 0.8222 | Val Loss: 1.8486\n",
      "Epoch 3 | Train Loss: 0.5577 | Val Loss: 1.2384\n",
      "Epoch 4 | Train Loss: 0.4069 | Val Loss: 0.8720\n",
      "Epoch 5 | Train Loss: 0.3026 | Val Loss: 0.6815\n",
      "Epoch 6 | Train Loss: 0.2408 | Val Loss: 0.5980\n",
      "Epoch 7 | Train Loss: 0.1942 | Val Loss: 0.5686\n",
      "Epoch 8 | Train Loss: 0.1750 | Val Loss: 0.5485\n",
      "Epoch 9 | Train Loss: 0.1571 | Val Loss: 0.5239\n",
      "Epoch 10 | Train Loss: 0.1489 | Val Loss: 0.4728\n",
      "Epoch 11 | Train Loss: 0.1395 | Val Loss: 0.4626\n",
      "Epoch 12 | Train Loss: 0.1308 | Val Loss: 0.4323\n",
      "Epoch 13 | Train Loss: 0.1232 | Val Loss: 0.4146\n",
      "Epoch 14 | Train Loss: 0.1170 | Val Loss: 0.4129\n",
      "Epoch 15 | Train Loss: 0.1125 | Val Loss: 0.3932\n",
      "Epoch 16 | Train Loss: 0.1091 | Val Loss: 0.3873\n",
      "Epoch 17 | Train Loss: 0.1087 | Val Loss: 0.3634\n",
      "Epoch 18 | Train Loss: 0.1058 | Val Loss: 0.3643\n",
      "Epoch 19 | Train Loss: 0.0979 | Val Loss: 0.3513\n",
      "Epoch 20 | Train Loss: 0.0998 | Val Loss: 0.3271\n",
      "Epoch 21 | Train Loss: 0.0986 | Val Loss: 0.3153\n",
      "Epoch 22 | Train Loss: 0.0958 | Val Loss: 0.3133\n",
      "Epoch 23 | Train Loss: 0.0947 | Val Loss: 0.3098\n",
      "Epoch 24 | Train Loss: 0.0936 | Val Loss: 0.2968\n",
      "Epoch 25 | Train Loss: 0.0919 | Val Loss: 0.2773\n",
      "Epoch 26 | Train Loss: 0.0898 | Val Loss: 0.2769\n",
      "Epoch 27 | Train Loss: 0.0868 | Val Loss: 0.2821\n",
      "Epoch 28 | Train Loss: 0.0854 | Val Loss: 0.2715\n",
      "Epoch 29 | Train Loss: 0.0811 | Val Loss: 0.2571\n",
      "Epoch 30 | Train Loss: 0.0839 | Val Loss: 0.2426\n",
      "Training complete!\n",
      "Site: site-1, Test MAE: 0.1264\n",
      "Site: site-10, Test MAE: 0.1551\n",
      "Site: site-11, Test MAE: 0.1090\n",
      "Site: site-12, Test MAE: 0.1554\n",
      "Site: site-2, Test MAE: 0.1178\n",
      "Site: site-3, Test MAE: 0.1056\n",
      "Site: site-4, Test MAE: 0.1495\n",
      "Site: site-5, Test MAE: 0.1096\n",
      "Site: site-6, Test MAE: 0.1342\n",
      "Site: site-7, Test MAE: 0.1108\n",
      "Site: site-8, Test MAE: 0.1408\n",
      "Site: site-9, Test MAE: 0.1677\n",
      "Site-wise MAE for Air Quality at horizon 1: [0.1264263540506363, 0.155133455991745, 0.10903588682413101, 0.15535800158977509, 0.11775435507297516, 0.1056462824344635, 0.14946943521499634, 0.10961588472127914, 0.1342429220676422, 0.11078104376792908, 0.14082269370555878, 0.1676824688911438]\n",
      "Average MAE for Air Quality at horizon 1: 0.1318\n",
      "✅ Results saved to forecasting_results.json\n",
      "\n",
      "--- Running Global Model for Horizon: 2 ---\n",
      "Site: site-1 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-10 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-11 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-12 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-2 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-3 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-4 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-5 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-6 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-7 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-8 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-9 | Train: 1119 | Validation: 279 | Test: 350\n",
      "data loader length: [33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33]\n",
      "max dataloader length: 33 epoch iteration: 396\n",
      "Total trainable parameters: 1042\n",
      "Epoch 1 | Train Loss: 1.0095 | Val Loss: 1.9972\n",
      "Epoch 2 | Train Loss: 0.6767 | Val Loss: 1.2676\n",
      "Epoch 3 | Train Loss: 0.4610 | Val Loss: 0.9617\n",
      "Epoch 4 | Train Loss: 0.3751 | Val Loss: 0.7775\n",
      "Epoch 5 | Train Loss: 0.3071 | Val Loss: 0.6543\n",
      "Epoch 6 | Train Loss: 0.2531 | Val Loss: 0.5858\n",
      "Epoch 7 | Train Loss: 0.2207 | Val Loss: 0.5574\n",
      "Epoch 8 | Train Loss: 0.1987 | Val Loss: 0.5378\n",
      "Epoch 9 | Train Loss: 0.1820 | Val Loss: 0.5396\n",
      "Epoch 10 | Train Loss: 0.1686 | Val Loss: 0.5243\n",
      "Epoch 11 | Train Loss: 0.1600 | Val Loss: 0.5113\n",
      "Epoch 12 | Train Loss: 0.1515 | Val Loss: 0.5031\n",
      "Epoch 13 | Train Loss: 0.1472 | Val Loss: 0.5036\n",
      "Epoch 14 | Train Loss: 0.1405 | Val Loss: 0.4977\n",
      "Epoch 15 | Train Loss: 0.1369 | Val Loss: 0.4930\n",
      "Epoch 16 | Train Loss: 0.1335 | Val Loss: 0.4959\n",
      "Epoch 17 | Train Loss: 0.1285 | Val Loss: 0.4727\n",
      "Epoch 18 | Train Loss: 0.1282 | Val Loss: 0.4773\n",
      "Epoch 19 | Train Loss: 0.1224 | Val Loss: 0.4829\n",
      "Epoch 20 | Train Loss: 0.1206 | Val Loss: 0.4782\n",
      "Epoch 21 | Train Loss: 0.1211 | Val Loss: 0.4765\n",
      "Epoch 22 | Train Loss: 0.1157 | Val Loss: 0.4446\n",
      "Epoch 23 | Train Loss: 0.1150 | Val Loss: 0.4445\n",
      "Epoch 24 | Train Loss: 0.1136 | Val Loss: 0.4294\n",
      "Epoch 25 | Train Loss: 0.1137 | Val Loss: 0.4452\n",
      "Epoch 26 | Train Loss: 0.1109 | Val Loss: 0.4202\n",
      "Epoch 27 | Train Loss: 0.1124 | Val Loss: 0.4180\n",
      "Epoch 28 | Train Loss: 0.1082 | Val Loss: 0.4079\n",
      "Epoch 29 | Train Loss: 0.1053 | Val Loss: 0.4127\n",
      "Epoch 30 | Train Loss: 0.1055 | Val Loss: 0.4055\n",
      "Training complete!\n",
      "Site: site-1, Test MAE: 0.1531\n",
      "Site: site-10, Test MAE: 0.1789\n",
      "Site: site-11, Test MAE: 0.1327\n",
      "Site: site-12, Test MAE: 0.1846\n",
      "Site: site-2, Test MAE: 0.1544\n",
      "Site: site-3, Test MAE: 0.1349\n",
      "Site: site-4, Test MAE: 0.1817\n",
      "Site: site-5, Test MAE: 0.1296\n",
      "Site: site-6, Test MAE: 0.1627\n",
      "Site: site-7, Test MAE: 0.1414\n",
      "Site: site-8, Test MAE: 0.1802\n",
      "Site: site-9, Test MAE: 0.1918\n",
      "Site-wise MAE for Air Quality at horizon 2: [0.15314480662345886, 0.1789129376411438, 0.13271482288837433, 0.18462176620960236, 0.15444643795490265, 0.1349191814661026, 0.18171972036361694, 0.1296289563179016, 0.1626746654510498, 0.14144833385944366, 0.18024203181266785, 0.19179296493530273]\n",
      "Average MAE for Air Quality at horizon 2: 0.1605\n",
      "✅ Results saved to forecasting_results.json\n",
      "\n",
      "--- Running Global Model for Horizon: 4 ---\n",
      "Site: site-1 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-10 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-11 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-12 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-2 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-3 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-4 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-5 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-6 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-7 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-8 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-9 | Train: 1119 | Validation: 279 | Test: 350\n",
      "data loader length: [33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33]\n",
      "max dataloader length: 33 epoch iteration: 396\n",
      "Total trainable parameters: 1060\n",
      "Epoch 1 | Train Loss: 1.0291 | Val Loss: 2.6155\n",
      "Epoch 2 | Train Loss: 0.9046 | Val Loss: 1.9532\n",
      "Epoch 3 | Train Loss: 0.6591 | Val Loss: 1.4046\n",
      "Epoch 4 | Train Loss: 0.5266 | Val Loss: 1.0945\n",
      "Epoch 5 | Train Loss: 0.4211 | Val Loss: 0.8902\n",
      "Epoch 6 | Train Loss: 0.3544 | Val Loss: 0.7689\n",
      "Epoch 7 | Train Loss: 0.2905 | Val Loss: 0.7248\n",
      "Epoch 8 | Train Loss: 0.2591 | Val Loss: 0.7242\n",
      "Epoch 9 | Train Loss: 0.2287 | Val Loss: 0.7462\n",
      "Epoch 10 | Train Loss: 0.2114 | Val Loss: 0.7773\n",
      "Epoch 11 | Train Loss: 0.1988 | Val Loss: 0.7901\n",
      "Epoch 12 | Train Loss: 0.1896 | Val Loss: 0.7968\n",
      "Epoch 13 | Train Loss: 0.1847 | Val Loss: 0.8193\n",
      "Epoch 14 | Train Loss: 0.1801 | Val Loss: 0.8128\n",
      "Epoch 15 | Train Loss: 0.1697 | Val Loss: 0.8031\n",
      "Epoch 16 | Train Loss: 0.1700 | Val Loss: 0.8114\n",
      "Epoch 17 | Train Loss: 0.1694 | Val Loss: 0.8012\n",
      "Epoch 18 | Train Loss: 0.1658 | Val Loss: 0.7836\n",
      "Epoch 19 | Train Loss: 0.1614 | Val Loss: 0.7739\n",
      "Epoch 20 | Train Loss: 0.1664 | Val Loss: 0.7731\n",
      "Epoch 21 | Train Loss: 0.1579 | Val Loss: 0.7736\n",
      "Epoch 22 | Train Loss: 0.1573 | Val Loss: 0.7623\n",
      "Epoch 23 | Train Loss: 0.1552 | Val Loss: 0.7730\n",
      "Epoch 24 | Train Loss: 0.1540 | Val Loss: 0.7494\n",
      "Epoch 25 | Train Loss: 0.1515 | Val Loss: 0.7571\n",
      "Epoch 26 | Train Loss: 0.1473 | Val Loss: 0.7543\n",
      "Epoch 27 | Train Loss: 0.1506 | Val Loss: 0.7366\n",
      "Epoch 28 | Train Loss: 0.1467 | Val Loss: 0.7316\n",
      "Epoch 29 | Train Loss: 0.1418 | Val Loss: 0.7082\n",
      "Epoch 30 | Train Loss: 0.1433 | Val Loss: 0.7090\n",
      "Training complete!\n",
      "Site: site-1, Test MAE: 0.2164\n",
      "Site: site-10, Test MAE: 0.2427\n",
      "Site: site-11, Test MAE: 0.1996\n",
      "Site: site-12, Test MAE: 0.2734\n",
      "Site: site-2, Test MAE: 0.1988\n",
      "Site: site-3, Test MAE: 0.1901\n",
      "Site: site-4, Test MAE: 0.2500\n",
      "Site: site-5, Test MAE: 0.1872\n",
      "Site: site-6, Test MAE: 0.2190\n",
      "Site: site-7, Test MAE: 0.1941\n",
      "Site: site-8, Test MAE: 0.2963\n",
      "Site: site-9, Test MAE: 0.2676\n",
      "Site-wise MAE for Air Quality at horizon 4: [0.21636544167995453, 0.2426638901233673, 0.1996232569217682, 0.27340397238731384, 0.19881004095077515, 0.19012737274169922, 0.24998968839645386, 0.18715731799602509, 0.21899916231632233, 0.19411474466323853, 0.29628869891166687, 0.2676299810409546]\n",
      "Average MAE for Air Quality at horizon 4: 0.2279\n",
      "✅ Results saved to forecasting_results.json\n",
      "\n",
      "--- Running Global Model for Horizon: 8 ---\n",
      "Site: site-1 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-10 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-11 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-12 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-2 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-3 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-4 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-5 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-6 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-7 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-8 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-9 | Train: 1119 | Validation: 279 | Test: 350\n",
      "data loader length: [33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33]\n",
      "max dataloader length: 33 epoch iteration: 396\n",
      "Total trainable parameters: 1096\n",
      "Epoch 1 | Train Loss: 1.0711 | Val Loss: 2.4941\n",
      "Epoch 2 | Train Loss: 0.9155 | Val Loss: 2.0369\n",
      "Epoch 3 | Train Loss: 0.7148 | Val Loss: 1.5727\n",
      "Epoch 4 | Train Loss: 0.6009 | Val Loss: 1.2837\n",
      "Epoch 5 | Train Loss: 0.5037 | Val Loss: 1.0757\n",
      "Epoch 6 | Train Loss: 0.4332 | Val Loss: 0.9518\n",
      "Epoch 7 | Train Loss: 0.3738 | Val Loss: 0.8967\n",
      "Epoch 8 | Train Loss: 0.3304 | Val Loss: 0.9060\n",
      "Epoch 9 | Train Loss: 0.3076 | Val Loss: 0.9396\n",
      "Epoch 10 | Train Loss: 0.2854 | Val Loss: 0.9695\n",
      "Epoch 11 | Train Loss: 0.2714 | Val Loss: 1.0062\n",
      "Epoch 12 | Train Loss: 0.2576 | Val Loss: 1.0438\n",
      "Epoch 13 | Train Loss: 0.2490 | Val Loss: 1.0952\n",
      "Epoch 14 | Train Loss: 0.2453 | Val Loss: 1.1289\n",
      "Epoch 15 | Train Loss: 0.2428 | Val Loss: 1.1524\n",
      "Epoch 16 | Train Loss: 0.2379 | Val Loss: 1.1808\n",
      "Epoch 17 | Train Loss: 0.2393 | Val Loss: 1.2040\n",
      "Epoch 18 | Train Loss: 0.2351 | Val Loss: 1.2178\n",
      "Epoch 19 | Train Loss: 0.2301 | Val Loss: 1.2207\n",
      "Epoch 20 | Train Loss: 0.2274 | Val Loss: 1.2337\n",
      "Epoch 21 | Train Loss: 0.2248 | Val Loss: 1.2314\n",
      "Epoch 22 | Train Loss: 0.2335 | Val Loss: 1.2241\n",
      "Epoch 23 | Train Loss: 0.2169 | Val Loss: 1.2245\n",
      "Epoch 24 | Train Loss: 0.2263 | Val Loss: 1.2192\n",
      "Epoch 25 | Train Loss: 0.2194 | Val Loss: 1.2137\n",
      "Epoch 26 | Train Loss: 0.2208 | Val Loss: 1.2107\n",
      "Epoch 27 | Train Loss: 0.2222 | Val Loss: 1.2045\n",
      "Epoch 28 | Train Loss: 0.2102 | Val Loss: 1.1972\n",
      "Epoch 29 | Train Loss: 0.2173 | Val Loss: 1.1892\n",
      "Epoch 30 | Train Loss: 0.2132 | Val Loss: 1.1788\n",
      "Training complete!\n",
      "Site: site-1, Test MAE: 0.2948\n",
      "Site: site-10, Test MAE: 0.3403\n",
      "Site: site-11, Test MAE: 0.2934\n",
      "Site: site-12, Test MAE: 0.3769\n",
      "Site: site-2, Test MAE: 0.2792\n",
      "Site: site-3, Test MAE: 0.2848\n",
      "Site: site-4, Test MAE: 0.3469\n",
      "Site: site-5, Test MAE: 0.2511\n",
      "Site: site-6, Test MAE: 0.2832\n",
      "Site: site-7, Test MAE: 0.2971\n",
      "Site: site-8, Test MAE: 0.3966\n",
      "Site: site-9, Test MAE: 0.3830\n",
      "Site-wise MAE for Air Quality at horizon 8: [0.29477351903915405, 0.3403070271015167, 0.29340115189552307, 0.3768588900566101, 0.27920350432395935, 0.2847980856895447, 0.34693679213523865, 0.2510881721973419, 0.2832397222518921, 0.2970837950706482, 0.3965747058391571, 0.38300764560699463]\n",
      "Average MAE for Air Quality at horizon 8: 0.3189\n",
      "✅ Results saved to forecasting_results.json\n",
      "\n",
      "--- Running Global Model for Horizon: 16 ---\n",
      "Site: site-1 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-10 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-11 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-12 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-2 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-3 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-4 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-5 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-6 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-7 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-8 | Train: 1119 | Validation: 279 | Test: 350\n",
      "Site: site-9 | Train: 1119 | Validation: 279 | Test: 350\n",
      "data loader length: [33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33]\n",
      "max dataloader length: 33 epoch iteration: 396\n",
      "Total trainable parameters: 1168\n",
      "Epoch 1 | Train Loss: 1.0553 | Val Loss: 2.2711\n",
      "Epoch 2 | Train Loss: 0.9089 | Val Loss: 1.8692\n",
      "Epoch 3 | Train Loss: 0.7605 | Val Loss: 1.5820\n",
      "Epoch 4 | Train Loss: 0.6563 | Val Loss: 1.3946\n",
      "Epoch 5 | Train Loss: 0.5879 | Val Loss: 1.2867\n",
      "Epoch 6 | Train Loss: 0.5303 | Val Loss: 1.2250\n",
      "Epoch 7 | Train Loss: 0.4785 | Val Loss: 1.2078\n",
      "Epoch 8 | Train Loss: 0.4482 | Val Loss: 1.2204\n",
      "Epoch 9 | Train Loss: 0.4236 | Val Loss: 1.2407\n",
      "Epoch 10 | Train Loss: 0.3933 | Val Loss: 1.2779\n",
      "Epoch 11 | Train Loss: 0.3754 | Val Loss: 1.3153\n",
      "Epoch 12 | Train Loss: 0.3622 | Val Loss: 1.3626\n",
      "Epoch 13 | Train Loss: 0.3553 | Val Loss: 1.4106\n",
      "Epoch 14 | Train Loss: 0.3476 | Val Loss: 1.4504\n",
      "Epoch 15 | Train Loss: 0.3336 | Val Loss: 1.4997\n",
      "Epoch 16 | Train Loss: 0.3323 | Val Loss: 1.5337\n",
      "Epoch 17 | Train Loss: 0.3248 | Val Loss: 1.5714\n",
      "Epoch 18 | Train Loss: 0.3326 | Val Loss: 1.5947\n",
      "Epoch 19 | Train Loss: 0.3203 | Val Loss: 1.6223\n",
      "Epoch 20 | Train Loss: 0.3184 | Val Loss: 1.6358\n",
      "Epoch 21 | Train Loss: 0.3140 | Val Loss: 1.6573\n",
      "Epoch 22 | Train Loss: 0.3147 | Val Loss: 1.6747\n",
      "Epoch 23 | Train Loss: 0.3099 | Val Loss: 1.6905\n",
      "Epoch 24 | Train Loss: 0.3147 | Val Loss: 1.6943\n",
      "Epoch 25 | Train Loss: 0.3064 | Val Loss: 1.6949\n",
      "Epoch 26 | Train Loss: 0.3084 | Val Loss: 1.7224\n",
      "Epoch 27 | Train Loss: 0.3033 | Val Loss: 1.7276\n",
      "Epoch 28 | Train Loss: 0.3080 | Val Loss: 1.7290\n",
      "Epoch 29 | Train Loss: 0.3117 | Val Loss: 1.7351\n",
      "Epoch 30 | Train Loss: 0.3070 | Val Loss: 1.7458\n",
      "Training complete!\n",
      "Site: site-1, Test MAE: 0.3697\n",
      "Site: site-10, Test MAE: 0.4705\n",
      "Site: site-11, Test MAE: 0.3632\n",
      "Site: site-12, Test MAE: 0.5204\n",
      "Site: site-2, Test MAE: 0.3115\n",
      "Site: site-3, Test MAE: 0.3733\n",
      "Site: site-4, Test MAE: 0.4672\n",
      "Site: site-5, Test MAE: 0.3436\n",
      "Site: site-6, Test MAE: 0.3427\n",
      "Site: site-7, Test MAE: 0.3985\n",
      "Site: site-8, Test MAE: 0.5331\n",
      "Site: site-9, Test MAE: 0.5359\n",
      "Site-wise MAE for Air Quality at horizon 16: [0.36973392963409424, 0.47045081853866577, 0.3631502389907837, 0.5203918218612671, 0.3114663362503052, 0.373348593711853, 0.4671630859375, 0.34356236457824707, 0.3426806628704071, 0.3984526991844177, 0.5331188440322876, 0.5359499454498291]\n",
      "Average MAE for Air Quality at horizon 16: 0.4191\n",
      "✅ Results saved to forecasting_results.json\n",
      "\n",
      "==================== Dataset: Solar ====================\n",
      "\n",
      "--- Running Global Model for Horizon: 1 ---\n",
      "Site: solar_al | Train: 1328 | Validation: 332 | Test: 415\n",
      "Site: solar_fl | Train: 1328 | Validation: 332 | Test: 415\n",
      "Site: solar_il | Train: 1328 | Validation: 332 | Test: 415\n",
      "Site: solar_ks | Train: 1328 | Validation: 332 | Test: 415\n",
      "Site: solar_ma | Train: 1328 | Validation: 332 | Test: 415\n",
      "Site: solar_me | Train: 1328 | Validation: 332 | Test: 415\n",
      "data loader length: [40, 40, 40, 40, 40, 40]\n",
      "max dataloader length: 40 epoch iteration: 240\n",
      "Total trainable parameters: 1033\n",
      "Epoch 1 | Train Loss: 0.9268 | Val Loss: 1.1395\n",
      "Epoch 2 | Train Loss: 0.7139 | Val Loss: 0.7696\n",
      "Epoch 3 | Train Loss: 0.4434 | Val Loss: 0.4436\n",
      "Epoch 4 | Train Loss: 0.3046 | Val Loss: 0.3162\n",
      "Epoch 5 | Train Loss: 0.2483 | Val Loss: 0.2577\n",
      "Epoch 6 | Train Loss: 0.2152 | Val Loss: 0.2235\n",
      "Epoch 7 | Train Loss: 0.1917 | Val Loss: 0.1976\n",
      "Epoch 8 | Train Loss: 0.1747 | Val Loss: 0.1769\n",
      "Epoch 9 | Train Loss: 0.1649 | Val Loss: 0.1633\n",
      "Epoch 10 | Train Loss: 0.1557 | Val Loss: 0.1481\n",
      "Epoch 11 | Train Loss: 0.1491 | Val Loss: 0.1387\n",
      "Epoch 12 | Train Loss: 0.1418 | Val Loss: 0.1296\n",
      "Epoch 13 | Train Loss: 0.1350 | Val Loss: 0.1233\n",
      "Epoch 14 | Train Loss: 0.1283 | Val Loss: 0.1165\n",
      "Epoch 15 | Train Loss: 0.1304 | Val Loss: 0.1118\n",
      "Epoch 16 | Train Loss: 0.1228 | Val Loss: 0.1056\n",
      "Epoch 17 | Train Loss: 0.1205 | Val Loss: 0.1013\n",
      "Epoch 18 | Train Loss: 0.1165 | Val Loss: 0.0979\n",
      "Epoch 19 | Train Loss: 0.1166 | Val Loss: 0.0942\n",
      "Epoch 20 | Train Loss: 0.1112 | Val Loss: 0.0905\n",
      "Epoch 21 | Train Loss: 0.1068 | Val Loss: 0.0881\n",
      "Epoch 22 | Train Loss: 0.1026 | Val Loss: 0.0845\n",
      "Epoch 23 | Train Loss: 0.1046 | Val Loss: 0.0829\n",
      "Epoch 24 | Train Loss: 0.0999 | Val Loss: 0.0798\n",
      "Epoch 25 | Train Loss: 0.0997 | Val Loss: 0.0772\n",
      "Epoch 26 | Train Loss: 0.0965 | Val Loss: 0.0759\n",
      "Epoch 27 | Train Loss: 0.0989 | Val Loss: 0.0736\n",
      "Epoch 28 | Train Loss: 0.0944 | Val Loss: 0.0718\n",
      "Epoch 29 | Train Loss: 0.0885 | Val Loss: 0.0698\n",
      "Epoch 30 | Train Loss: 0.0875 | Val Loss: 0.0690\n",
      "Training complete!\n",
      "Site: solar_al, Test MAE: 0.1720\n",
      "Site: solar_fl, Test MAE: 0.1361\n",
      "Site: solar_il, Test MAE: 0.0965\n",
      "Site: solar_ks, Test MAE: 0.1185\n",
      "Site: solar_ma, Test MAE: 0.1774\n",
      "Site: solar_me, Test MAE: 0.1440\n",
      "Site-wise MAE for Solar at horizon 1: [0.17196407914161682, 0.1360730528831482, 0.09651246666908264, 0.11845164000988007, 0.17735780775547028, 0.1440015584230423]\n",
      "Average MAE for Solar at horizon 1: 0.1407\n",
      "✅ Results saved to forecasting_results.json\n",
      "\n",
      "--- Running Global Model for Horizon: 2 ---\n",
      "Site: solar_al | Train: 1328 | Validation: 332 | Test: 415\n",
      "Site: solar_fl | Train: 1328 | Validation: 332 | Test: 415\n",
      "Site: solar_il | Train: 1328 | Validation: 332 | Test: 415\n",
      "Site: solar_ks | Train: 1328 | Validation: 332 | Test: 415\n",
      "Site: solar_ma | Train: 1328 | Validation: 332 | Test: 415\n",
      "Site: solar_me | Train: 1328 | Validation: 332 | Test: 415\n",
      "data loader length: [40, 40, 40, 40, 40, 40]\n",
      "max dataloader length: 40 epoch iteration: 240\n",
      "Total trainable parameters: 1042\n",
      "Epoch 1 | Train Loss: 0.9243 | Val Loss: 1.1311\n",
      "Epoch 2 | Train Loss: 0.7751 | Val Loss: 0.8618\n",
      "Epoch 3 | Train Loss: 0.5552 | Val Loss: 0.5650\n",
      "Epoch 4 | Train Loss: 0.3876 | Val Loss: 0.3973\n",
      "Epoch 5 | Train Loss: 0.3053 | Val Loss: 0.3175\n",
      "Epoch 6 | Train Loss: 0.2648 | Val Loss: 0.2678\n",
      "Epoch 7 | Train Loss: 0.2296 | Val Loss: 0.2298\n",
      "Epoch 8 | Train Loss: 0.2040 | Val Loss: 0.2027\n",
      "Epoch 9 | Train Loss: 0.1827 | Val Loss: 0.1818\n",
      "Epoch 10 | Train Loss: 0.1697 | Val Loss: 0.1644\n",
      "Epoch 11 | Train Loss: 0.1607 | Val Loss: 0.1517\n",
      "Epoch 12 | Train Loss: 0.1532 | Val Loss: 0.1397\n",
      "Epoch 13 | Train Loss: 0.1469 | Val Loss: 0.1302\n",
      "Epoch 14 | Train Loss: 0.1383 | Val Loss: 0.1222\n",
      "Epoch 15 | Train Loss: 0.1322 | Val Loss: 0.1158\n",
      "Epoch 16 | Train Loss: 0.1290 | Val Loss: 0.1107\n",
      "Epoch 17 | Train Loss: 0.1269 | Val Loss: 0.1062\n",
      "Epoch 18 | Train Loss: 0.1244 | Val Loss: 0.1029\n",
      "Epoch 19 | Train Loss: 0.1240 | Val Loss: 0.1004\n",
      "Epoch 20 | Train Loss: 0.1201 | Val Loss: 0.0979\n",
      "Epoch 21 | Train Loss: 0.1188 | Val Loss: 0.0950\n",
      "Epoch 22 | Train Loss: 0.1168 | Val Loss: 0.0927\n",
      "Epoch 23 | Train Loss: 0.1130 | Val Loss: 0.0911\n",
      "Epoch 24 | Train Loss: 0.1148 | Val Loss: 0.0890\n",
      "Epoch 25 | Train Loss: 0.1099 | Val Loss: 0.0872\n",
      "Epoch 26 | Train Loss: 0.1071 | Val Loss: 0.0848\n",
      "Epoch 27 | Train Loss: 0.1101 | Val Loss: 0.0839\n",
      "Epoch 28 | Train Loss: 0.1058 | Val Loss: 0.0820\n",
      "Epoch 29 | Train Loss: 0.1031 | Val Loss: 0.0805\n",
      "Epoch 30 | Train Loss: 0.1028 | Val Loss: 0.0796\n",
      "Training complete!\n",
      "Site: solar_al, Test MAE: 0.1789\n",
      "Site: solar_fl, Test MAE: 0.1255\n",
      "Site: solar_il, Test MAE: 0.1085\n",
      "Site: solar_ks, Test MAE: 0.1374\n",
      "Site: solar_ma, Test MAE: 0.1854\n",
      "Site: solar_me, Test MAE: 0.1586\n",
      "Site-wise MAE for Solar at horizon 2: [0.1789330542087555, 0.1255398392677307, 0.10847330093383789, 0.1373826116323471, 0.18538020551204681, 0.15863972902297974]\n",
      "Average MAE for Solar at horizon 2: 0.1491\n",
      "✅ Results saved to forecasting_results.json\n",
      "\n",
      "--- Running Global Model for Horizon: 4 ---\n",
      "Site: solar_al | Train: 1328 | Validation: 332 | Test: 415\n",
      "Site: solar_fl | Train: 1328 | Validation: 332 | Test: 415\n",
      "Site: solar_il | Train: 1328 | Validation: 332 | Test: 415\n",
      "Site: solar_ks | Train: 1328 | Validation: 332 | Test: 415\n",
      "Site: solar_ma | Train: 1328 | Validation: 332 | Test: 415\n",
      "Site: solar_me | Train: 1328 | Validation: 332 | Test: 415\n",
      "data loader length: [40, 40, 40, 40, 40, 40]\n",
      "max dataloader length: 40 epoch iteration: 240\n",
      "Total trainable parameters: 1060\n",
      "Epoch 1 | Train Loss: 0.9701 | Val Loss: 1.1886\n",
      "Epoch 2 | Train Loss: 0.8229 | Val Loss: 0.9490\n",
      "Epoch 3 | Train Loss: 0.6613 | Val Loss: 0.7608\n",
      "Epoch 4 | Train Loss: 0.5427 | Val Loss: 0.6248\n",
      "Epoch 5 | Train Loss: 0.4487 | Val Loss: 0.5149\n",
      "Epoch 6 | Train Loss: 0.3810 | Val Loss: 0.4341\n",
      "Epoch 7 | Train Loss: 0.3264 | Val Loss: 0.3642\n",
      "Epoch 8 | Train Loss: 0.2835 | Val Loss: 0.3142\n",
      "Epoch 9 | Train Loss: 0.2536 | Val Loss: 0.2767\n",
      "Epoch 10 | Train Loss: 0.2335 | Val Loss: 0.2481\n",
      "Epoch 11 | Train Loss: 0.2169 | Val Loss: 0.2269\n",
      "Epoch 12 | Train Loss: 0.2055 | Val Loss: 0.2099\n",
      "Epoch 13 | Train Loss: 0.1932 | Val Loss: 0.1988\n",
      "Epoch 14 | Train Loss: 0.1868 | Val Loss: 0.1882\n",
      "Epoch 15 | Train Loss: 0.1822 | Val Loss: 0.1796\n",
      "Epoch 16 | Train Loss: 0.1813 | Val Loss: 0.1736\n",
      "Epoch 17 | Train Loss: 0.1756 | Val Loss: 0.1667\n",
      "Epoch 18 | Train Loss: 0.1683 | Val Loss: 0.1614\n",
      "Epoch 19 | Train Loss: 0.1675 | Val Loss: 0.1554\n",
      "Epoch 20 | Train Loss: 0.1659 | Val Loss: 0.1519\n",
      "Epoch 21 | Train Loss: 0.1630 | Val Loss: 0.1479\n",
      "Epoch 22 | Train Loss: 0.1641 | Val Loss: 0.1447\n",
      "Epoch 23 | Train Loss: 0.1558 | Val Loss: 0.1408\n",
      "Epoch 24 | Train Loss: 0.1535 | Val Loss: 0.1374\n",
      "Epoch 25 | Train Loss: 0.1509 | Val Loss: 0.1352\n",
      "Epoch 26 | Train Loss: 0.1513 | Val Loss: 0.1321\n",
      "Epoch 27 | Train Loss: 0.1499 | Val Loss: 0.1300\n",
      "Epoch 28 | Train Loss: 0.1477 | Val Loss: 0.1279\n",
      "Epoch 29 | Train Loss: 0.1441 | Val Loss: 0.1263\n",
      "Epoch 30 | Train Loss: 0.1417 | Val Loss: 0.1249\n",
      "Training complete!\n",
      "Site: solar_al, Test MAE: 0.2054\n",
      "Site: solar_fl, Test MAE: 0.1794\n",
      "Site: solar_il, Test MAE: 0.1623\n",
      "Site: solar_ks, Test MAE: 0.1559\n",
      "Site: solar_ma, Test MAE: 0.2611\n",
      "Site: solar_me, Test MAE: 0.2065\n",
      "Site-wise MAE for Solar at horizon 4: [0.20544376969337463, 0.17935489118099213, 0.1623421460390091, 0.15594300627708435, 0.2611420452594757, 0.20647096633911133]\n",
      "Average MAE for Solar at horizon 4: 0.1951\n",
      "✅ Results saved to forecasting_results.json\n",
      "\n",
      "--- Running Global Model for Horizon: 8 ---\n",
      "Site: solar_al | Train: 1328 | Validation: 332 | Test: 415\n",
      "Site: solar_fl | Train: 1328 | Validation: 332 | Test: 415\n",
      "Site: solar_il | Train: 1328 | Validation: 332 | Test: 415\n",
      "Site: solar_ks | Train: 1328 | Validation: 332 | Test: 415\n",
      "Site: solar_ma | Train: 1328 | Validation: 332 | Test: 415\n",
      "Site: solar_me | Train: 1328 | Validation: 332 | Test: 415\n",
      "data loader length: [40, 40, 40, 40, 40, 40]\n",
      "max dataloader length: 40 epoch iteration: 240\n",
      "Total trainable parameters: 1096\n",
      "Epoch 1 | Train Loss: 1.0339 | Val Loss: 1.3195\n",
      "Epoch 2 | Train Loss: 0.9469 | Val Loss: 1.1326\n",
      "Epoch 3 | Train Loss: 0.7436 | Val Loss: 0.8548\n",
      "Epoch 4 | Train Loss: 0.5733 | Val Loss: 0.6687\n",
      "Epoch 5 | Train Loss: 0.4847 | Val Loss: 0.5485\n",
      "Epoch 6 | Train Loss: 0.4053 | Val Loss: 0.4568\n",
      "Epoch 7 | Train Loss: 0.3498 | Val Loss: 0.3873\n",
      "Epoch 8 | Train Loss: 0.3012 | Val Loss: 0.3373\n",
      "Epoch 9 | Train Loss: 0.2786 | Val Loss: 0.2995\n",
      "Epoch 10 | Train Loss: 0.2526 | Val Loss: 0.2732\n",
      "Epoch 11 | Train Loss: 0.2406 | Val Loss: 0.2542\n",
      "Epoch 12 | Train Loss: 0.2291 | Val Loss: 0.2375\n",
      "Epoch 13 | Train Loss: 0.2217 | Val Loss: 0.2256\n",
      "Epoch 14 | Train Loss: 0.2156 | Val Loss: 0.2147\n",
      "Epoch 15 | Train Loss: 0.2084 | Val Loss: 0.2068\n",
      "Epoch 16 | Train Loss: 0.2029 | Val Loss: 0.2010\n",
      "Epoch 17 | Train Loss: 0.2051 | Val Loss: 0.1942\n",
      "Epoch 18 | Train Loss: 0.1959 | Val Loss: 0.1895\n",
      "Epoch 19 | Train Loss: 0.1976 | Val Loss: 0.1848\n",
      "Epoch 20 | Train Loss: 0.1930 | Val Loss: 0.1818\n",
      "Epoch 21 | Train Loss: 0.1901 | Val Loss: 0.1759\n",
      "Epoch 22 | Train Loss: 0.1886 | Val Loss: 0.1731\n",
      "Epoch 23 | Train Loss: 0.1809 | Val Loss: 0.1702\n",
      "Epoch 24 | Train Loss: 0.1829 | Val Loss: 0.1667\n",
      "Epoch 25 | Train Loss: 0.1847 | Val Loss: 0.1644\n",
      "Epoch 26 | Train Loss: 0.1796 | Val Loss: 0.1614\n",
      "Epoch 27 | Train Loss: 0.1795 | Val Loss: 0.1589\n",
      "Epoch 28 | Train Loss: 0.1751 | Val Loss: 0.1570\n",
      "Epoch 29 | Train Loss: 0.1770 | Val Loss: 0.1557\n",
      "Epoch 30 | Train Loss: 0.1713 | Val Loss: 0.1522\n",
      "Training complete!\n",
      "Site: solar_al, Test MAE: 0.2371\n",
      "Site: solar_fl, Test MAE: 0.1841\n",
      "Site: solar_il, Test MAE: 0.1683\n",
      "Site: solar_ks, Test MAE: 0.1811\n",
      "Site: solar_ma, Test MAE: 0.2702\n",
      "Site: solar_me, Test MAE: 0.2339\n",
      "Site-wise MAE for Solar at horizon 8: [0.23710116744041443, 0.18413695693016052, 0.16834358870983124, 0.18110008537769318, 0.2702410817146301, 0.2338757961988449]\n",
      "Average MAE for Solar at horizon 8: 0.2125\n",
      "✅ Results saved to forecasting_results.json\n",
      "\n",
      "--- Running Global Model for Horizon: 16 ---\n",
      "Site: solar_al | Train: 1328 | Validation: 332 | Test: 415\n",
      "Site: solar_fl | Train: 1328 | Validation: 332 | Test: 415\n",
      "Site: solar_il | Train: 1328 | Validation: 332 | Test: 415\n",
      "Site: solar_ks | Train: 1328 | Validation: 332 | Test: 415\n",
      "Site: solar_ma | Train: 1328 | Validation: 332 | Test: 415\n",
      "Site: solar_me | Train: 1328 | Validation: 332 | Test: 415\n",
      "data loader length: [40, 40, 40, 40, 40, 40]\n",
      "max dataloader length: 40 epoch iteration: 240\n",
      "Total trainable parameters: 1168\n",
      "Epoch 1 | Train Loss: 1.0276 | Val Loss: 1.3368\n",
      "Epoch 2 | Train Loss: 0.9162 | Val Loss: 1.1254\n",
      "Epoch 3 | Train Loss: 0.7345 | Val Loss: 0.8494\n",
      "Epoch 4 | Train Loss: 0.5785 | Val Loss: 0.6663\n",
      "Epoch 5 | Train Loss: 0.4945 | Val Loss: 0.5637\n",
      "Epoch 6 | Train Loss: 0.4331 | Val Loss: 0.4916\n",
      "Epoch 7 | Train Loss: 0.3825 | Val Loss: 0.4354\n",
      "Epoch 8 | Train Loss: 0.3606 | Val Loss: 0.3916\n",
      "Epoch 9 | Train Loss: 0.3367 | Val Loss: 0.3637\n",
      "Epoch 10 | Train Loss: 0.3159 | Val Loss: 0.3450\n",
      "Epoch 11 | Train Loss: 0.3030 | Val Loss: 0.3259\n",
      "Epoch 12 | Train Loss: 0.2977 | Val Loss: 0.3141\n",
      "Epoch 13 | Train Loss: 0.2922 | Val Loss: 0.2998\n",
      "Epoch 14 | Train Loss: 0.2892 | Val Loss: 0.2951\n",
      "Epoch 15 | Train Loss: 0.2801 | Val Loss: 0.2829\n",
      "Epoch 16 | Train Loss: 0.2782 | Val Loss: 0.2784\n",
      "Epoch 17 | Train Loss: 0.2727 | Val Loss: 0.2707\n",
      "Epoch 18 | Train Loss: 0.2710 | Val Loss: 0.2640\n",
      "Epoch 19 | Train Loss: 0.2661 | Val Loss: 0.2598\n",
      "Epoch 20 | Train Loss: 0.2621 | Val Loss: 0.2558\n",
      "Epoch 21 | Train Loss: 0.2659 | Val Loss: 0.2534\n",
      "Epoch 22 | Train Loss: 0.2602 | Val Loss: 0.2477\n",
      "Epoch 23 | Train Loss: 0.2593 | Val Loss: 0.2406\n",
      "Epoch 24 | Train Loss: 0.2589 | Val Loss: 0.2390\n",
      "Epoch 25 | Train Loss: 0.2532 | Val Loss: 0.2351\n",
      "Epoch 26 | Train Loss: 0.2543 | Val Loss: 0.2319\n",
      "Epoch 27 | Train Loss: 0.2511 | Val Loss: 0.2313\n",
      "Epoch 28 | Train Loss: 0.2485 | Val Loss: 0.2288\n",
      "Epoch 29 | Train Loss: 0.2470 | Val Loss: 0.2277\n",
      "Epoch 30 | Train Loss: 0.2430 | Val Loss: 0.2192\n",
      "Training complete!\n",
      "Site: solar_al, Test MAE: 0.2674\n",
      "Site: solar_fl, Test MAE: 0.2178\n",
      "Site: solar_il, Test MAE: 0.2761\n",
      "Site: solar_ks, Test MAE: 0.2373\n",
      "Site: solar_ma, Test MAE: 0.4118\n",
      "Site: solar_me, Test MAE: 0.3131\n",
      "Site-wise MAE for Solar at horizon 16: [0.2673788070678711, 0.21776780486106873, 0.2760947048664093, 0.2372891753911972, 0.411827951669693, 0.3131026029586792]\n",
      "Average MAE for Solar at horizon 16: 0.2872\n",
      "✅ Results saved to forecasting_results.json\n",
      "\n",
      "==================== Dataset: Crypto ====================\n",
      "\n",
      "--- Running Global Model for Horizon: 1 ---\n",
      "Site: crypto-0 | Train: 1152 | Validation: 288 | Test: 361\n",
      "Site: crypto-1 | Train: 1152 | Validation: 288 | Test: 361\n",
      "Site: crypto-2 | Train: 1152 | Validation: 288 | Test: 361\n",
      "Site: crypto-3 | Train: 901 | Validation: 225 | Test: 282\n",
      "Site: crypto-4 | Train: 1152 | Validation: 288 | Test: 361\n",
      "data loader length: [35, 35, 35, 27, 35]\n",
      "max dataloader length: 35 epoch iteration: 175\n",
      "Total trainable parameters: 1065\n",
      "Epoch 1 | Train Loss: 1.0343 | Val Loss: 0.9781\n",
      "Epoch 2 | Train Loss: 0.9011 | Val Loss: 0.8321\n",
      "Epoch 3 | Train Loss: 0.7311 | Val Loss: 0.6651\n",
      "Epoch 4 | Train Loss: 0.5657 | Val Loss: 0.4670\n",
      "Epoch 5 | Train Loss: 0.4038 | Val Loss: 0.2809\n",
      "Epoch 6 | Train Loss: 0.2763 | Val Loss: 0.1559\n",
      "Epoch 7 | Train Loss: 0.1976 | Val Loss: 0.0927\n",
      "Epoch 8 | Train Loss: 0.1493 | Val Loss: 0.0633\n",
      "Epoch 9 | Train Loss: 0.1279 | Val Loss: 0.0451\n",
      "Epoch 10 | Train Loss: 0.1144 | Val Loss: 0.0351\n",
      "Epoch 11 | Train Loss: 0.0898 | Val Loss: 0.0302\n",
      "Epoch 12 | Train Loss: 0.0870 | Val Loss: 0.0273\n",
      "Epoch 13 | Train Loss: 0.0834 | Val Loss: 0.0258\n",
      "Epoch 14 | Train Loss: 0.0811 | Val Loss: 0.0245\n",
      "Epoch 15 | Train Loss: 0.0789 | Val Loss: 0.0236\n",
      "Epoch 16 | Train Loss: 0.0719 | Val Loss: 0.0237\n",
      "Epoch 17 | Train Loss: 0.0749 | Val Loss: 0.0231\n",
      "Epoch 18 | Train Loss: 0.0772 | Val Loss: 0.0224\n",
      "Epoch 19 | Train Loss: 0.0668 | Val Loss: 0.0220\n",
      "Epoch 20 | Train Loss: 0.0689 | Val Loss: 0.0224\n",
      "Epoch 21 | Train Loss: 0.0676 | Val Loss: 0.0218\n",
      "Epoch 22 | Train Loss: 0.0693 | Val Loss: 0.0214\n",
      "Epoch 23 | Train Loss: 0.0625 | Val Loss: 0.0210\n",
      "Epoch 24 | Train Loss: 0.0610 | Val Loss: 0.0205\n",
      "Epoch 25 | Train Loss: 0.0639 | Val Loss: 0.0198\n",
      "Epoch 26 | Train Loss: 0.0690 | Val Loss: 0.0195\n",
      "Epoch 27 | Train Loss: 0.0634 | Val Loss: 0.0194\n",
      "Epoch 28 | Train Loss: 0.0659 | Val Loss: 0.0188\n",
      "Epoch 29 | Train Loss: 0.0497 | Val Loss: 0.0183\n",
      "Epoch 30 | Train Loss: 0.0588 | Val Loss: 0.0183\n",
      "Training complete!\n",
      "Site: crypto-0, Test MAE: 0.6550\n",
      "Site: crypto-1, Test MAE: 0.1117\n",
      "Site: crypto-2, Test MAE: 0.0525\n",
      "Site: crypto-3, Test MAE: 0.0178\n",
      "Site: crypto-4, Test MAE: 0.0778\n",
      "Site-wise MAE for Crypto at horizon 1: [0.6549774408340454, 0.11172433197498322, 0.05246827006340027, 0.01777750812470913, 0.07778311520814896]\n",
      "Average MAE for Crypto at horizon 1: 0.1829\n",
      "✅ Results saved to forecasting_results.json\n",
      "\n",
      "--- Running Global Model for Horizon: 2 ---\n",
      "Site: crypto-0 | Train: 1152 | Validation: 288 | Test: 361\n",
      "Site: crypto-1 | Train: 1152 | Validation: 288 | Test: 361\n",
      "Site: crypto-2 | Train: 1152 | Validation: 288 | Test: 361\n",
      "Site: crypto-3 | Train: 901 | Validation: 225 | Test: 282\n",
      "Site: crypto-4 | Train: 1152 | Validation: 288 | Test: 361\n",
      "data loader length: [34, 34, 34, 27, 34]\n",
      "max dataloader length: 34 epoch iteration: 170\n",
      "Total trainable parameters: 1074\n",
      "Epoch 1 | Train Loss: 0.9040 | Val Loss: 0.8731\n",
      "Epoch 2 | Train Loss: 0.7859 | Val Loss: 0.7074\n",
      "Epoch 3 | Train Loss: 0.6152 | Val Loss: 0.4805\n",
      "Epoch 4 | Train Loss: 0.4314 | Val Loss: 0.2795\n",
      "Epoch 5 | Train Loss: 0.3040 | Val Loss: 0.1880\n",
      "Epoch 6 | Train Loss: 0.2395 | Val Loss: 0.1503\n",
      "Epoch 7 | Train Loss: 0.1981 | Val Loss: 0.1234\n",
      "Epoch 8 | Train Loss: 0.1771 | Val Loss: 0.0980\n",
      "Epoch 9 | Train Loss: 0.1542 | Val Loss: 0.0773\n",
      "Epoch 10 | Train Loss: 0.1360 | Val Loss: 0.0627\n",
      "Epoch 11 | Train Loss: 0.1222 | Val Loss: 0.0509\n",
      "Epoch 12 | Train Loss: 0.1121 | Val Loss: 0.0433\n",
      "Epoch 13 | Train Loss: 0.1066 | Val Loss: 0.0367\n",
      "Epoch 14 | Train Loss: 0.0914 | Val Loss: 0.0323\n",
      "Epoch 15 | Train Loss: 0.0863 | Val Loss: 0.0303\n",
      "Epoch 16 | Train Loss: 0.0806 | Val Loss: 0.0273\n",
      "Epoch 17 | Train Loss: 0.0861 | Val Loss: 0.0263\n",
      "Epoch 18 | Train Loss: 0.0846 | Val Loss: 0.0246\n",
      "Epoch 19 | Train Loss: 0.0746 | Val Loss: 0.0233\n",
      "Epoch 20 | Train Loss: 0.0662 | Val Loss: 0.0228\n",
      "Epoch 21 | Train Loss: 0.0695 | Val Loss: 0.0235\n",
      "Epoch 22 | Train Loss: 0.0601 | Val Loss: 0.0221\n",
      "Epoch 23 | Train Loss: 0.0646 | Val Loss: 0.0221\n",
      "Epoch 24 | Train Loss: 0.0664 | Val Loss: 0.0209\n",
      "Epoch 25 | Train Loss: 0.0680 | Val Loss: 0.0213\n",
      "Epoch 26 | Train Loss: 0.0658 | Val Loss: 0.0199\n",
      "Epoch 27 | Train Loss: 0.0592 | Val Loss: 0.0199\n",
      "Epoch 28 | Train Loss: 0.0642 | Val Loss: 0.0202\n",
      "Epoch 29 | Train Loss: 0.0693 | Val Loss: 0.0193\n",
      "Epoch 30 | Train Loss: 0.0685 | Val Loss: 0.0183\n",
      "Training complete!\n",
      "Site: crypto-0, Test MAE: 0.6670\n",
      "Site: crypto-1, Test MAE: 0.1295\n",
      "Site: crypto-2, Test MAE: 0.0506\n",
      "Site: crypto-3, Test MAE: 0.0517\n",
      "Site: crypto-4, Test MAE: 0.0709\n",
      "Site-wise MAE for Crypto at horizon 2: [0.6670147776603699, 0.12948022782802582, 0.05062025040388107, 0.0516720674932003, 0.07092400640249252]\n",
      "Average MAE for Crypto at horizon 2: 0.1939\n",
      "✅ Results saved to forecasting_results.json\n",
      "\n",
      "--- Running Global Model for Horizon: 4 ---\n",
      "Site: crypto-0 | Train: 1152 | Validation: 288 | Test: 361\n",
      "Site: crypto-1 | Train: 1152 | Validation: 288 | Test: 361\n",
      "Site: crypto-2 | Train: 1152 | Validation: 288 | Test: 361\n",
      "Site: crypto-3 | Train: 901 | Validation: 225 | Test: 282\n",
      "Site: crypto-4 | Train: 1152 | Validation: 288 | Test: 361\n",
      "data loader length: [34, 34, 34, 27, 34]\n",
      "max dataloader length: 34 epoch iteration: 170\n",
      "Total trainable parameters: 1092\n",
      "Epoch 1 | Train Loss: 1.0183 | Val Loss: 0.5290\n",
      "Epoch 2 | Train Loss: 0.9675 | Val Loss: 0.5194\n",
      "Epoch 3 | Train Loss: 0.8711 | Val Loss: 0.4717\n",
      "Epoch 4 | Train Loss: 0.7137 | Val Loss: 0.3822\n",
      "Epoch 5 | Train Loss: 0.5279 | Val Loss: 0.3055\n",
      "Epoch 6 | Train Loss: 0.4016 | Val Loss: 0.2600\n",
      "Epoch 7 | Train Loss: 0.3296 | Val Loss: 0.2305\n",
      "Epoch 8 | Train Loss: 0.2823 | Val Loss: 0.1996\n",
      "Epoch 9 | Train Loss: 0.2387 | Val Loss: 0.1734\n",
      "Epoch 10 | Train Loss: 0.2119 | Val Loss: 0.1491\n",
      "Epoch 11 | Train Loss: 0.1836 | Val Loss: 0.1278\n",
      "Epoch 12 | Train Loss: 0.1705 | Val Loss: 0.1101\n",
      "Epoch 13 | Train Loss: 0.1502 | Val Loss: 0.0956\n",
      "Epoch 14 | Train Loss: 0.1357 | Val Loss: 0.0838\n",
      "Epoch 15 | Train Loss: 0.1116 | Val Loss: 0.0761\n",
      "Epoch 16 | Train Loss: 0.1103 | Val Loss: 0.0665\n",
      "Epoch 17 | Train Loss: 0.1123 | Val Loss: 0.0613\n",
      "Epoch 18 | Train Loss: 0.0968 | Val Loss: 0.0558\n",
      "Epoch 19 | Train Loss: 0.0939 | Val Loss: 0.0520\n",
      "Epoch 20 | Train Loss: 0.0877 | Val Loss: 0.0474\n",
      "Epoch 21 | Train Loss: 0.0884 | Val Loss: 0.0442\n",
      "Epoch 22 | Train Loss: 0.0931 | Val Loss: 0.0411\n",
      "Epoch 23 | Train Loss: 0.0794 | Val Loss: 0.0398\n",
      "Epoch 24 | Train Loss: 0.0858 | Val Loss: 0.0393\n",
      "Epoch 25 | Train Loss: 0.0856 | Val Loss: 0.0362\n",
      "Epoch 26 | Train Loss: 0.0746 | Val Loss: 0.0340\n",
      "Epoch 27 | Train Loss: 0.0685 | Val Loss: 0.0341\n",
      "Epoch 28 | Train Loss: 0.0666 | Val Loss: 0.0330\n",
      "Epoch 29 | Train Loss: 0.0810 | Val Loss: 0.0327\n",
      "Epoch 30 | Train Loss: 0.0694 | Val Loss: 0.0314\n",
      "Training complete!\n",
      "Site: crypto-0, Test MAE: 0.6913\n",
      "Site: crypto-1, Test MAE: 0.1805\n",
      "Site: crypto-2, Test MAE: 0.0619\n",
      "Site: crypto-3, Test MAE: 0.1613\n",
      "Site: crypto-4, Test MAE: 0.0776\n",
      "Site-wise MAE for Crypto at horizon 4: [0.6912604570388794, 0.18048539757728577, 0.061934106051921844, 0.16130472719669342, 0.07763727009296417]\n",
      "Average MAE for Crypto at horizon 4: 0.2345\n",
      "✅ Results saved to forecasting_results.json\n",
      "\n",
      "--- Running Global Model for Horizon: 8 ---\n",
      "Site: crypto-0 | Train: 1152 | Validation: 288 | Test: 361\n",
      "Site: crypto-1 | Train: 1152 | Validation: 288 | Test: 361\n",
      "Site: crypto-2 | Train: 1152 | Validation: 288 | Test: 361\n",
      "Site: crypto-3 | Train: 901 | Validation: 225 | Test: 282\n",
      "Site: crypto-4 | Train: 1152 | Validation: 288 | Test: 361\n",
      "data loader length: [34, 34, 34, 26, 34]\n",
      "max dataloader length: 34 epoch iteration: 170\n",
      "Total trainable parameters: 1128\n",
      "Epoch 1 | Train Loss: 1.0030 | Val Loss: 0.5915\n",
      "Epoch 2 | Train Loss: 0.9498 | Val Loss: 0.5510\n",
      "Epoch 3 | Train Loss: 0.8413 | Val Loss: 0.4783\n",
      "Epoch 4 | Train Loss: 0.6468 | Val Loss: 0.3839\n",
      "Epoch 5 | Train Loss: 0.4706 | Val Loss: 0.3172\n",
      "Epoch 6 | Train Loss: 0.3879 | Val Loss: 0.2706\n",
      "Epoch 7 | Train Loss: 0.3293 | Val Loss: 0.2389\n",
      "Epoch 8 | Train Loss: 0.2808 | Val Loss: 0.2060\n",
      "Epoch 9 | Train Loss: 0.2541 | Val Loss: 0.1725\n",
      "Epoch 10 | Train Loss: 0.2112 | Val Loss: 0.1421\n",
      "Epoch 11 | Train Loss: 0.2023 | Val Loss: 0.1173\n",
      "Epoch 12 | Train Loss: 0.1762 | Val Loss: 0.0975\n",
      "Epoch 13 | Train Loss: 0.1492 | Val Loss: 0.0830\n",
      "Epoch 14 | Train Loss: 0.1504 | Val Loss: 0.0756\n",
      "Epoch 15 | Train Loss: 0.1325 | Val Loss: 0.0681\n",
      "Epoch 16 | Train Loss: 0.1292 | Val Loss: 0.0625\n",
      "Epoch 17 | Train Loss: 0.1216 | Val Loss: 0.0591\n",
      "Epoch 18 | Train Loss: 0.1237 | Val Loss: 0.0560\n",
      "Epoch 19 | Train Loss: 0.1159 | Val Loss: 0.0538\n",
      "Epoch 20 | Train Loss: 0.1187 | Val Loss: 0.0526\n",
      "Epoch 21 | Train Loss: 0.1044 | Val Loss: 0.0518\n",
      "Epoch 22 | Train Loss: 0.1109 | Val Loss: 0.0488\n",
      "Epoch 23 | Train Loss: 0.1077 | Val Loss: 0.0464\n",
      "Epoch 24 | Train Loss: 0.1164 | Val Loss: 0.0451\n",
      "Epoch 25 | Train Loss: 0.1072 | Val Loss: 0.0446\n",
      "Epoch 26 | Train Loss: 0.1098 | Val Loss: 0.0432\n",
      "Epoch 27 | Train Loss: 0.0965 | Val Loss: 0.0427\n",
      "Epoch 28 | Train Loss: 0.0959 | Val Loss: 0.0417\n",
      "Epoch 29 | Train Loss: 0.1102 | Val Loss: 0.0401\n",
      "Epoch 30 | Train Loss: 0.0994 | Val Loss: 0.0424\n",
      "Training complete!\n",
      "Site: crypto-0, Test MAE: 0.7204\n",
      "Site: crypto-1, Test MAE: 0.1887\n",
      "Site: crypto-2, Test MAE: 0.0873\n",
      "Site: crypto-3, Test MAE: 0.1720\n",
      "Site: crypto-4, Test MAE: 0.1261\n",
      "Site-wise MAE for Crypto at horizon 8: [0.7203575372695923, 0.18866771459579468, 0.08734045177698135, 0.17198096215724945, 0.12607741355895996]\n",
      "Average MAE for Crypto at horizon 8: 0.2589\n",
      "✅ Results saved to forecasting_results.json\n",
      "\n",
      "--- Running Global Model for Horizon: 16 ---\n",
      "Site: crypto-0 | Train: 1152 | Validation: 288 | Test: 361\n",
      "Site: crypto-1 | Train: 1152 | Validation: 288 | Test: 361\n",
      "Site: crypto-2 | Train: 1152 | Validation: 288 | Test: 361\n",
      "Site: crypto-3 | Train: 901 | Validation: 225 | Test: 282\n",
      "Site: crypto-4 | Train: 1152 | Validation: 288 | Test: 361\n",
      "data loader length: [34, 34, 34, 26, 34]\n",
      "max dataloader length: 34 epoch iteration: 170\n",
      "Total trainable parameters: 1200\n",
      "Epoch 1 | Train Loss: 1.0167 | Val Loss: 0.6231\n",
      "Epoch 2 | Train Loss: 0.9672 | Val Loss: 0.5894\n",
      "Epoch 3 | Train Loss: 0.8751 | Val Loss: 0.5030\n",
      "Epoch 4 | Train Loss: 0.7131 | Val Loss: 0.3851\n",
      "Epoch 5 | Train Loss: 0.5715 | Val Loss: 0.3083\n",
      "Epoch 6 | Train Loss: 0.4696 | Val Loss: 0.2627\n",
      "Epoch 7 | Train Loss: 0.3906 | Val Loss: 0.2334\n",
      "Epoch 8 | Train Loss: 0.3475 | Val Loss: 0.2081\n",
      "Epoch 9 | Train Loss: 0.2966 | Val Loss: 0.1858\n",
      "Epoch 10 | Train Loss: 0.2594 | Val Loss: 0.1644\n",
      "Epoch 11 | Train Loss: 0.2324 | Val Loss: 0.1484\n",
      "Epoch 12 | Train Loss: 0.2206 | Val Loss: 0.1368\n",
      "Epoch 13 | Train Loss: 0.1953 | Val Loss: 0.1249\n",
      "Epoch 14 | Train Loss: 0.1805 | Val Loss: 0.1219\n",
      "Epoch 15 | Train Loss: 0.1667 | Val Loss: 0.1114\n",
      "Epoch 16 | Train Loss: 0.1542 | Val Loss: 0.1048\n",
      "Epoch 17 | Train Loss: 0.1473 | Val Loss: 0.0997\n",
      "Epoch 18 | Train Loss: 0.1544 | Val Loss: 0.0962\n",
      "Epoch 19 | Train Loss: 0.1414 | Val Loss: 0.0933\n",
      "Epoch 20 | Train Loss: 0.1418 | Val Loss: 0.0874\n",
      "Epoch 21 | Train Loss: 0.1486 | Val Loss: 0.0838\n",
      "Epoch 22 | Train Loss: 0.1455 | Val Loss: 0.0805\n",
      "Epoch 23 | Train Loss: 0.1360 | Val Loss: 0.0789\n",
      "Epoch 24 | Train Loss: 0.1255 | Val Loss: 0.0774\n",
      "Epoch 25 | Train Loss: 0.1444 | Val Loss: 0.0747\n",
      "Epoch 26 | Train Loss: 0.1315 | Val Loss: 0.0733\n",
      "Epoch 27 | Train Loss: 0.1281 | Val Loss: 0.0732\n",
      "Epoch 28 | Train Loss: 0.1345 | Val Loss: 0.0697\n",
      "Epoch 29 | Train Loss: 0.1155 | Val Loss: 0.0697\n",
      "Epoch 30 | Train Loss: 0.1236 | Val Loss: 0.0682\n",
      "Training complete!\n",
      "Site: crypto-0, Test MAE: 0.8748\n",
      "Site: crypto-1, Test MAE: 0.1944\n",
      "Site: crypto-2, Test MAE: 0.1028\n",
      "Site: crypto-3, Test MAE: 0.2038\n",
      "Site: crypto-4, Test MAE: 0.1263\n",
      "Site-wise MAE for Crypto at horizon 16: [0.8748385310173035, 0.19438974559307098, 0.1028306633234024, 0.2037663757801056, 0.12625427544116974]\n",
      "Average MAE for Crypto at horizon 16: 0.3004\n",
      "✅ Results saved to forecasting_results.json\n",
      "\n",
      "🏆 All Global LSTM Forecasting Model experiments are complete! 🏆\n"
     ]
    }
   ],
   "source": [
    "run_global_model_experiment_lstm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "centralized",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
